{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ce77c0",
   "metadata": {},
   "source": [
    "### P E A R L: P R O B A B I L I S T I C - E M B E D D I N G - F O R - A C T O R - C R I T I C - R L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960bc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ce38a",
   "metadata": {},
   "source": [
    "### L O G G I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db862be",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/PEARL')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8628bb35",
   "metadata": {},
   "source": [
    "### D E V I C E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e61fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0570f",
   "metadata": {},
   "source": [
    "### H E L P E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab13188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n",
    "\n",
    "def safe_stack(x):\n",
    "    \n",
    "    return torch.stack(x).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41341eb",
   "metadata": {},
   "source": [
    "### M E T A - E N V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9628f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta_Walker_2d:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.base_env = gym.make('Walker2d-v5')\n",
    "        self.target_velocity = 1.0\n",
    "        \n",
    "    def sample_tasks(self, num_tasks):\n",
    "        \n",
    "        self.tasks = []\n",
    "        \n",
    "        for _ in range(num_tasks):\n",
    "            \n",
    "            gravity = np.random.uniform(5.0, 15.0)\n",
    "            torso_mass = np.random.uniform(1.0, 5.0)\n",
    "            target_velocity = np.random.uniform(0.5, 3.0)\n",
    "            \n",
    "            self.tasks.append((gravity, torso_mass, target_velocity))\n",
    "            \n",
    "        return self.tasks\n",
    "    \n",
    "    def set_task(self, task):\n",
    "        \n",
    "        raw_env = self.base_env.unwrapped\n",
    "        raw_env.model.opt.gravity[-1] = -task[0]\n",
    "        raw_env.model.body_mass[1] = task[1]\n",
    "        self.target_velocity = task[2]\n",
    "        \n",
    "    def reset(self, task_idx = 0):\n",
    "        \n",
    "        self.set_task(self.tasks[task_idx])\n",
    "        obs, _ = self.base_env.reset()\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        next_obs, reward, terminated, truncated, info = self.base_env.step(action)\n",
    "        done = terminated | truncated\n",
    "        raw_env = self.base_env.unwrapped\n",
    "        vel = raw_env.data.qvel[0]\n",
    "        reward -= 0.5 * abs(vel - self.target_velocity)\n",
    "        \n",
    "        return next_obs, reward, done, info\n",
    "    \n",
    "    def get_number(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = self.base_env.action_space.high[0]\n",
    "        reward_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, reward_dim\n",
    "    \n",
    "    def close(self):\n",
    "        \n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b2eaf",
   "metadata": {},
   "source": [
    "### T E S T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d27b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs shape: (17,)\n",
      "state dim: 17 | action dim: 6 | max action: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = Meta_Walker_2d()\n",
    "\n",
    "env.sample_tasks(num_tasks = 4)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "print(f'obs shape: {obs.shape}')\n",
    "\n",
    "state_dim, action_dim, max_action, reward_dim = env.get_number()\n",
    "\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38a922",
   "metadata": {},
   "source": [
    "### A S S E M B L Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da7d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 64\n",
    "head_2 = 128\n",
    "head_3 = 128\n",
    "head_4 = 64\n",
    "\n",
    "latent_dim = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d900d2",
   "metadata": {},
   "source": [
    "### P O S T E R I O R - q [ z | c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc87958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pearl_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, reward_dim = reward_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, latent_dim = latent_dim):\n",
    "        super(pearl_encoder, self).__init__()\n",
    "        \n",
    "        # find the posterior\n",
    "        \n",
    "        self.encoder_net = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(2 * state_dim + action_dim + reward_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_1),\n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, latent_dim)\n",
    "        self.log_std = nn.Linear(head_4, latent_dim)\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, module):\n",
    "            \n",
    "        if isinstance(module, nn.Linear):\n",
    "                \n",
    "            nn.init.kaiming_normal_(module.weight, a = 0, nonlinearity = 'relu')\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, state, action, reward, next_state, sample = False, reduce = True):\n",
    "        \n",
    "        # concat\n",
    "        \n",
    "        cat = torch.cat([state, action, reward, next_state], dim = -1)\n",
    "        \n",
    "        # encoder net\n",
    "        \n",
    "        x = self.encoder_net(cat)\n",
    "        \n",
    "        # mu and log std head\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, -5 , 2)\n",
    "        \n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        if sample:\n",
    "        \n",
    "            z = mu + std * eps\n",
    "            \n",
    "        else :\n",
    "            \n",
    "            z = mu\n",
    "            \n",
    "        if reduce:\n",
    "            \n",
    "            z = z.mean(dim = 0, keepdim = True)\n",
    "            mu = mu.mean(dim = 0, keepdim = True)\n",
    "            std = std.mean(dim = 0, keepdim = True)\n",
    "        \n",
    "        return z, mu, log_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad661433",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87eb3082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearl_encoder(\n",
      "  (encoder_net): Sequential(\n",
      "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (log_std): Linear(in_features=64, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "PEARL_ENCODER = pearl_encoder().to(device)\n",
    "\n",
    "print(PEARL_ENCODER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6f87e",
   "metadata": {},
   "source": [
    "### P E A R L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a70bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, latent_dim = latent_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action):\n",
    "        super(actor_critic, self).__init__()\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # create mlp\n",
    "        \n",
    "        def create_mlp(input_dim):\n",
    "            \n",
    "            process = nn.Sequential(\n",
    "                \n",
    "            nn.Linear(input_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "                \n",
    "            nn.LayerNorm(head_1),\n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "                \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "                \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU()\n",
    "            ) \n",
    "        \n",
    "            return process\n",
    "        \n",
    "        # actor and critic heads\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        self.critic_head = nn.Linear(head_4, 1)\n",
    "        self.critic_head_2 = nn.Linear(head_4, 1)\n",
    "        \n",
    "        # specific actor and critic mlp\n",
    "        \n",
    "        self.actor_mlp = create_mlp(input_dim = state_dim + latent_dim)\n",
    "        self.critic_mlp = create_mlp(input_dim = state_dim + action_dim + latent_dim)\n",
    "        self.critic_mlp_2 = create_mlp(input_dim = state_dim + action_dim + latent_dim)\n",
    "        \n",
    "        # apply initialization\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, module):\n",
    "            \n",
    "        if isinstance(module, nn.Linear):\n",
    "                \n",
    "            nn.init.kaiming_normal_(module.weight, a = 0, nonlinearity = 'relu')\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(module.bias)        \n",
    "                \n",
    "    def actor_forward(self, state, latent_z, deterministic = False):\n",
    "        \n",
    "        # cat\n",
    "        \n",
    "        actor_ca = torch.cat([state, latent_z], dim = -1)\n",
    "        \n",
    "        # pass to mlp\n",
    "        \n",
    "        actor_pass = self.actor_mlp(actor_ca)\n",
    "        \n",
    "        # mu and log head\n",
    "        \n",
    "        mu = self.mu(actor_pass)\n",
    "        if deterministic: return mu\n",
    "\n",
    "        log_std = self.log_std(actor_pass)\n",
    "        log_std = torch.clamp(log_std, -5, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        sampled_z = dist.rsample()\n",
    "        log_prob = dist.log_prob(sampled_z)\n",
    "        tanh_Z =torch.tanh(sampled_z)\n",
    "        action = tanh_Z * self.max_action \n",
    "        \n",
    "        squash = (1 - tanh_Z.pow(2) + 1e-6).log()\n",
    "        log_prob = log_prob - squash\n",
    "        log_prob = log_prob.sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob, mu, log_std\n",
    "            \n",
    "    def critic_forward(self, state, action, latent_z):\n",
    "        \n",
    "        # cat\n",
    "        \n",
    "        critic_ca = torch.cat([state, action, latent_z], dim = -1)\n",
    "        \n",
    "        # pass to mlp\n",
    "        \n",
    "        critic_pass = self.critic_mlp(critic_ca)\n",
    "        critic_pass_2 = self.critic_mlp_2(critic_ca)\n",
    "        \n",
    "        # pass mlp output to heads\n",
    "        \n",
    "        critic_val = self.critic_head(critic_pass)\n",
    "        critic_val_2 = self.critic_head_2(critic_pass_2)\n",
    "        \n",
    "        \n",
    "        return critic_val, critic_val_2\n",
    "    \n",
    "    def forward(self, state, latent_z):\n",
    "        \n",
    "        action, log_prob, mu, log_std = self.actor_forward(state, latent_z)\n",
    "        \n",
    "        critic_val, critic_val_2 = self.critic_forward(state, action, latent_z)\n",
    "        \n",
    "        return action, log_prob, mu, log_std, critic_val, critic_val_2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9bf776",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0ab560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor_critic(\n",
      "  (mu): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (log_std): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (critic_head): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (critic_head_2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (actor_mlp): Sequential(\n",
      "    (0): Linear(in_features=81, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): SiLU()\n",
      "  )\n",
      "  (critic_mlp): Sequential(\n",
      "    (0): Linear(in_features=87, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): SiLU()\n",
      "  )\n",
      "  (critic_mlp_2): Sequential(\n",
      "    (0): Linear(in_features=87, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): SiLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "PEARL = actor_critic().to(device)\n",
    "\n",
    "print(PEARL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875ddd8",
   "metadata": {},
   "source": [
    "### T A R G E T - C R I T I C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "156b7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class target_critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, latent_dim = latent_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action):\n",
    "        super(target_critic, self).__init__()\n",
    "\n",
    " \n",
    "        def create_mlp(input_dim):\n",
    "                    \n",
    "            process = nn.Sequential(\n",
    "                        \n",
    "                nn.Linear(input_dim, head_1),\n",
    "                nn.SiLU(),\n",
    "                        \n",
    "                nn.LayerNorm(head_1),\n",
    "                nn.Linear(head_1, head_2),\n",
    "                nn.SiLU(),\n",
    "                        \n",
    "                nn.LayerNorm(head_2),\n",
    "                nn.Linear(head_2, head_3),\n",
    "                nn.SiLU(),\n",
    "                        \n",
    "                nn.LayerNorm(head_3),\n",
    "                nn.Linear(head_3, head_4),\n",
    "                nn.SiLU()\n",
    "            ) \n",
    "            \n",
    "            return process\n",
    "        \n",
    "        # critic heads\n",
    "        \n",
    "        self.critic_head_1 = nn.Linear(head_4, 1)\n",
    "        self.critic_head_2 = nn.Linear(head_4, 1)\n",
    "        \n",
    "        # initiate mlp\n",
    "        \n",
    "        self.critic_1 = create_mlp(input_dim = state_dim + action_dim + latent_dim)\n",
    "        self.critic_2 = create_mlp(input_dim = state_dim + action_dim + latent_dim)\n",
    "        \n",
    "        # Initialize\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, module):\n",
    "            \n",
    "        if isinstance(module, nn.Linear):\n",
    "                \n",
    "            nn.init.kaiming_normal_(module.weight, a = 0, nonlinearity = 'relu')\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, state, action, latent_z):\n",
    "        \n",
    "        critic_ca = torch.cat([state, action, latent_z], dim = -1)\n",
    "        \n",
    "        pass_1 = self.critic_1(critic_ca)\n",
    "        pass_2 = self.critic_2(critic_ca)\n",
    "        \n",
    "        q1 = self.critic_head_1(pass_1)\n",
    "        q2 = self.critic_head_2(pass_2)\n",
    "        \n",
    "        return q1, q2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd444369",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8901002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_critic(\n",
      "  (critic_head_1): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (critic_head_2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (critic_1): Sequential(\n",
      "    (0): Linear(in_features=87, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): SiLU()\n",
      "  )\n",
      "  (critic_2): Sequential(\n",
      "    (0): Linear(in_features=87, out_features=64, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): SiLU()\n",
      "    (8): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): SiLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "TARGET_CRITIC = target_critic().to(device)\n",
    "\n",
    "# Initialize to CRITIC \n",
    "\n",
    "TARGET_CRITIC.critic_1.load_state_dict(PEARL.critic_mlp.state_dict())\n",
    "TARGET_CRITIC.critic_2.load_state_dict(PEARL.critic_mlp_2.state_dict())\n",
    "TARGET_CRITIC.critic_head_1.load_state_dict(PEARL.critic_head.state_dict())\n",
    "TARGET_CRITIC.critic_head_2.load_state_dict(PEARL.critic_head_2.state_dict())\n",
    "\n",
    "\n",
    "print(TARGET_CRITIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f89da",
   "metadata": {},
   "source": [
    "### O P T I M I Z E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc24ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lr\n",
    "\n",
    "encoder_lr = 1e-4\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 5e-4\n",
    "\n",
    "T_max = 100\n",
    "\n",
    "# Shared optimizer\n",
    "\n",
    "actor_params = list(PEARL.actor_mlp.parameters()) + \\\n",
    "               list(PEARL.mu.parameters()) + \\\n",
    "               list(PEARL.log_std.parameters()) \n",
    "               \n",
    "               \n",
    "critic_params = list(PEARL.critic_mlp.parameters()) + \\\n",
    "                list(PEARL.critic_mlp_2.parameters()) + \\\n",
    "                list(PEARL.critic_head.parameters()) + \\\n",
    "                list(PEARL.critic_head_2.parameters())\n",
    "                \n",
    "# Optimizer\n",
    "                \n",
    "PEARL_OPTIMIZER = optim.AdamW([    \n",
    "                \n",
    "    {'params': critic_params, 'lr': critic_lr, 'weight_decay': 1e-6},\n",
    "    {'params': PEARL_ENCODER.parameters(), 'lr': encoder_lr, 'weight_decay': 1e-6},\n",
    "    {'params': actor_params, 'lr': actor_lr, 'weight_decay': 0},\n",
    "    \n",
    "])\n",
    "\n",
    "# PEARL SCHEDULER\n",
    "\n",
    "PEARL_SCHEDULER = optim.lr_scheduler.CosineAnnealingLR(PEARL_OPTIMIZER, T_max, eta_min = 1e-5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000229af",
   "metadata": {},
   "source": [
    "### B U F F E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce4d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_buffer:\n",
    "    \n",
    "    def __init__(self, max_episodes, env = env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.max_episodes = max_episodes\n",
    "        self.current_episode = []\n",
    "        self.episodes = []\n",
    "        \n",
    "    def add(self, state, action, log_probs, reward, done, next_state):\n",
    "        \n",
    "        # convert to tensor\n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = safe_tensor(action)\n",
    "        reward = safe_tensor(reward)\n",
    "        done = safe_tensor(done)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        log_probs = safe_tensor(log_probs)\n",
    "        \n",
    "        # add them\n",
    "        \n",
    "        self.current_episode.append({\n",
    "            \n",
    "            'states': state,\n",
    "            'actions': action,\n",
    "            'log_probs': log_probs,\n",
    "            'rewards': reward,\n",
    "            'dones': done,\n",
    "            'next_states': next_state\n",
    "        })\n",
    "        \n",
    "        if done.item() == 1:\n",
    "        \n",
    "            self.episodes.append(self.current_episode)\n",
    "            \n",
    "            self.current_episode = []\n",
    "            \n",
    "        if len(self.episodes) > self.max_episodes:\n",
    "                \n",
    "            self.episodes.pop(0)\n",
    "            \n",
    "    def sample(self, batch_size, fixed_length):\n",
    "        \n",
    "        #for _ in range(batch_size):\n",
    "        \n",
    "        segments = []\n",
    "        masks = []\n",
    "        \n",
    "        sampled_episodes = random.sample(self.episodes, k=min(batch_size, len(self.episodes)))\n",
    "        \n",
    "        for ep in sampled_episodes:\n",
    "        \n",
    "            if len(ep) >= fixed_length:\n",
    "                \n",
    "                seg = ep[:fixed_length]\n",
    "                mask = torch.ones(fixed_length, dtype=torch.float32)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                seg, mask = self.pad_episode(ep, fixed_length)\n",
    "                \n",
    "            segments.append(seg)\n",
    "            masks.append(mask)\n",
    "        \n",
    "        \n",
    "        def stack_field(x):\n",
    "        \n",
    "            return torch.stack([torch.stack([step[x] for step in seg]) for seg in segments]).to(device)\n",
    "            \n",
    "        batch = {\n",
    "                \n",
    "                'states': stack_field('states'),\n",
    "                'actions': stack_field('actions'),\n",
    "                'log_probs': stack_field('log_probs'),\n",
    "                'rewards': stack_field('rewards'),\n",
    "                'dones': stack_field('dones'),\n",
    "                'next_states': stack_field('next_states'),\n",
    "                'mask': torch.stack(masks).to(device)\n",
    "        }\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    def pad_episode(self, ep, fixed_length):\n",
    "        \n",
    "        pad_length = fixed_length - len(ep)\n",
    "        last = ep[-1]\n",
    "        \n",
    "        pad_step = {}\n",
    "        \n",
    "        for k,v in last.items():\n",
    "            \n",
    "            if torch.is_tensor(v):\n",
    "                \n",
    "                pad_step[k] = torch.zeros_like(v)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                pad_step[k] = v\n",
    "    \n",
    "        mask = torch.cat([\n",
    "            \n",
    "            torch.ones(len(ep), dtype = torch.float32),\n",
    "            torch.zeros(pad_length, dtype = torch.float32)\n",
    "        ])\n",
    "\n",
    "        return ep + [pad_step] * pad_length, mask\n",
    "    \n",
    "    def clear(self):\n",
    "        \n",
    "        self.episodes.clear()\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83d7e5",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04c0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 500\n",
    "\n",
    "buffer = meta_buffer(max_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62b584",
   "metadata": {},
   "source": [
    "### M E T A - E P I S O D E - R U N N E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0947032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_episode_runner:\n",
    "    \n",
    "    def __init__(self, max_episode_length, env = env, buffer = buffer, model = PEARL, encoder = PEARL_ENCODER):\n",
    "        \n",
    "        self.env = env\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.buffer = buffer\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def run(self, num_tasks):\n",
    "        \n",
    "        tasks = self.env.sample_tasks(num_tasks)\n",
    "        \n",
    "        for task in tasks:\n",
    "            \n",
    "            self.env.set_task(task)\n",
    "            \n",
    "            obs = self.env.reset()\n",
    "            obs = safe_tensor(obs)\n",
    "            if obs.dim() == 1: obs = obs.unsqueeze(0)\n",
    "            \n",
    "            context = []\n",
    "            episode_reward = 0.0\n",
    "\n",
    "            for _ in range(self.max_episode_length):\n",
    "                \n",
    "                latent_z = self.get_latent_from_context(context)\n",
    "                    \n",
    "                # Get action from policy\n",
    "                \n",
    "                action, log_prob, _, _ = self.model.actor_forward(obs, latent_z)\n",
    "                \n",
    "                # step the env\n",
    "                \n",
    "                action_np = action.detach().cpu().numpy()[0]\n",
    "                \n",
    "                next_obs, reward, done, _ = self.env.step(action_np)\n",
    "                \n",
    "                episode_reward += reward.item()\n",
    "                \n",
    "                # correct shapes\n",
    "                \n",
    "                next_obs, reward, done = self.shape_corrector(next_obs, reward, done)\n",
    "                \n",
    "                # save to buffer\n",
    "                \n",
    "                self.buffer.add(obs.squeeze(0), action.squeeze(0), log_prob.squeeze(0), reward.squeeze(0), done.squeeze(0), next_obs.squeeze(0))\n",
    "\n",
    "                # save to context\n",
    "                \n",
    "                context.append({\n",
    "                    \n",
    "                    'states': obs,\n",
    "                    'actions': action,\n",
    "                    'rewards': reward,\n",
    "                    'next_states': next_obs    \n",
    "                })\n",
    "                \n",
    "                obs = next_obs\n",
    "                \n",
    "                if done:\n",
    "                    \n",
    "                    break\n",
    "               \n",
    "            # log reward    \n",
    "                \n",
    "            #print(f\"Task done. Total reward: {episode_reward}\")\n",
    "                \n",
    "    def build_context_tensor(self, context):\n",
    "    \n",
    "        return {\n",
    "            \n",
    "            'states': torch.cat([e['states'] for e in context], dim = 0),\n",
    "            'actions': torch.cat([e['actions'] for e in context], dim = 0),\n",
    "            'rewards': torch.cat([e['rewards'] for e in context], dim = 0),\n",
    "            'next_states': torch.cat([e['next_states'] for e in context], dim = 0)\n",
    "        }\n",
    "        \n",
    "    def get_latent_from_context(self, context):\n",
    "        \n",
    "        if context:\n",
    "                    \n",
    "            ctx = self.build_context_tensor(context)\n",
    "                    \n",
    "            latent_z, _, _ = self.encoder(ctx['states'], ctx['actions'], ctx['rewards'], ctx['next_states'])\n",
    "                    \n",
    "        else:\n",
    "                    \n",
    "            latent_z = torch.zeros((1, latent_dim)).to(device)\n",
    "\n",
    "        return latent_z\n",
    "    \n",
    "    def shape_corrector(self, next_obs, reward, done):\n",
    "        \n",
    "        next_obs = safe_tensor(next_obs).unsqueeze(0)\n",
    "        reward = safe_tensor(reward)\n",
    "        done = safe_tensor(done)\n",
    "        reward = reward.view(-1, 1)\n",
    "        done = done.view(-1, 1)\n",
    "        \n",
    "        return next_obs, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f867f",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0706da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_length = 512\n",
    "\n",
    "META_RUNNER = meta_episode_runner(max_episode_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08a003",
   "metadata": {},
   "source": [
    "### L O S S - F U N C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a5e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    \n",
    "    def __init__(self, gamma, tau, entropy_scalar, kl_coef, action_dim = action_dim, PEARL = PEARL, PEARL_ENCODER = PEARL_ENCODER, PEARL_OPTIMIZER = PEARL_OPTIMIZER, PEARL_SCHEDULER = PEARL_SCHEDULER, TARGET_CRITIC = TARGET_CRITIC, buffer = buffer):\n",
    "        \n",
    "        # network\n",
    "        \n",
    "        self.encoder = PEARL_ENCODER\n",
    "        self.pearl = PEARL\n",
    "        self.target_critic = TARGET_CRITIC\n",
    "        \n",
    "        # optimizer and scheduler\n",
    "        \n",
    "        self.pearl_optimizer = PEARL_OPTIMIZER\n",
    "        self.pearl_scheduler = PEARL_SCHEDULER\n",
    "        \n",
    "        # hyper params\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.entropy_scalar = entropy_scalar\n",
    "        self.kl_coef = kl_coef\n",
    "    \n",
    "        \n",
    "        # buffer\n",
    "        \n",
    "        self.buffer = buffer\n",
    "        \n",
    "        # auto alpha\n",
    "        \n",
    "        self.log_alpha = torch.nn.Parameter(torch.tensor(np.log(0.2), device = device, requires_grad = True))\n",
    "        self.target_entropy = - action_dim * self.entropy_scalar\n",
    "        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr = 3e-4, weight_decay = 0)\n",
    "        \n",
    "    def compute_alpha(self):\n",
    "        \n",
    "        alpha = self.log_alpha.exp().detach()\n",
    "        alpha = alpha.clamp(min = 1e-3)\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def soft_update(self, target, source):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "                \n",
    "                target_param.data.copy_(param.data * self.tau + (1 - self.tau) * target_param.data)\n",
    "                \n",
    "    def critic_loss(self, q1, q2, target):\n",
    "        \n",
    "        loss_1 = F.mse_loss(q1, target)\n",
    "        loss_2 = F.mse_loss(q2, target)\n",
    "        \n",
    "        critic_loss = loss_1 + loss_2\n",
    "        \n",
    "        return critic_loss\n",
    "    \n",
    "    def actor_loss(self, detached_q1, detached_q2, alpha, old_log_probs):\n",
    "        \n",
    "        q_pi = torch.min(detached_q1, detached_q2)\n",
    "        \n",
    "        actor_loss = - (alpha * old_log_probs - q_pi).mean()\n",
    "        \n",
    "        return actor_loss\n",
    "    \n",
    "    def alpha_loss(self, detached_old_log_probs):\n",
    "        \n",
    "        alpha_loss = (self.log_alpha.exp() * (detached_old_log_probs - self.target_entropy)).mean()\n",
    "        \n",
    "        return alpha_loss\n",
    "    \n",
    "    def update(self, batch_size, fixed_length):\n",
    "        \n",
    "        batch = self.buffer.sample(batch_size, fixed_length)\n",
    "        \n",
    "        # sample\n",
    "        \n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards'] \n",
    "        dones = batch['dones']\n",
    "        next_states = batch['next_states']\n",
    "        \n",
    "        # compute target\n",
    "        \n",
    "        latent_z_actor, _, _ = self.encoder.forward(states, actions, rewards, next_states, reduce = False)\n",
    "        \n",
    "        latent_z_critic = latent_z_actor.detach().clone()\n",
    "        \n",
    "        # ''' i think we should also detach these transitions and rely on the gradients of the latent z or sepearate the paths clearly'''\n",
    "                              \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            alpha = self.compute_alpha()\n",
    "            \n",
    "            next_action, next_log_probs, _, _ = self.pearl.actor_forward(next_states, latent_z_critic)\n",
    "            \n",
    "            target_1, target_2 = self.target_critic.forward(next_states, next_action, latent_z_critic)\n",
    "            \n",
    "            target_val = torch.min(target_1, target_2)\n",
    "            \n",
    "            target = rewards + self.gamma * (1 - dones) * (target_val - alpha * next_log_probs)\n",
    "            \n",
    "        # compute current vals\n",
    "            \n",
    "        q1, q2 = self.pearl.critic_forward(states, actions.detach(), latent_z_critic)\n",
    "            \n",
    "        # cal critic loss\n",
    "        \n",
    "        critic_loss = self.critic_loss(q1 = q1, q2 = q2, target = target)       \n",
    "        \n",
    "        # cal actor loss\n",
    "        \n",
    "        new_actions, log_probs, _, _ = self.pearl.actor_forward(states, latent_z_actor.detach())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            actor_alpha = self.compute_alpha()\n",
    "        \n",
    "            detached_q1, detached_q2 = self.pearl.critic_forward(states, new_actions, latent_z_actor)\n",
    "        \n",
    "        actor_loss = self.actor_loss(detached_q1, detached_q2, actor_alpha, log_probs)\n",
    "        \n",
    "        # total loss\n",
    "        \n",
    "        pearl_loss = actor_loss + critic_loss\n",
    "                \n",
    "        # total agent loss\n",
    "        \n",
    "        self.pearl_optimizer.zero_grad()\n",
    "        pearl_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.pearl.parameters(), max_norm = 0.5)\n",
    "        self.pearl_optimizer.step()\n",
    "        self.pearl_scheduler.step()\n",
    "        \n",
    "        # cal alpha loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            _, log_probs_alpha, _, _ = self.pearl.actor_forward(states, latent_z_actor.detach())\n",
    "                \n",
    "        alpha_loss = self.alpha_loss(log_probs_alpha)\n",
    "        \n",
    "        # update alpha\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm = 0.5)\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        # soft update\n",
    "        \n",
    "        self.soft_update(target = self.target_critic.critic_1, source = self.pearl.critic_mlp)\n",
    "        self.soft_update(self.target_critic.critic_2, self.pearl.critic_mlp_2)\n",
    "        self.soft_update(self.target_critic.critic_head_1, self.pearl.critic_head)\n",
    "        self.soft_update(self.target_critic.critic_head_2, self.pearl.critic_head_2) \n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), alpha.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14da5e2",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56b64aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "entropy_scalar = 1.5\n",
    "kl_coef = 0.01\n",
    "\n",
    "# setup\n",
    "\n",
    "LOSS_FUNCTION = loss_func(gamma, tau, entropy_scalar, kl_coef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d3b43",
   "metadata": {},
   "source": [
    "### T R A I N I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13850c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import write\n",
    "\n",
    "\n",
    "def train_model(num_tasks, epochs, mini_batch, batch_size, fixed_length, META_RUNNER = META_RUNNER, LOSS_FUNCTION = LOSS_FUNCTION, buffer = buffer):\n",
    "    \n",
    "    PEARL.train()\n",
    "    PEARL_ENCODER.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_actor_loss, total_critic_loss = 0.0, 0.0\n",
    "        \n",
    "        for _ in range(mini_batch):\n",
    "            \n",
    "            buffer.clear()\n",
    "            \n",
    "            META_RUNNER.run(num_tasks)\n",
    "            \n",
    "            actor_loss, critic_loss, alpha = LOSS_FUNCTION.update(batch_size, fixed_length)\n",
    "            \n",
    "            total_actor_loss += actor_loss\n",
    "            total_critic_loss += critic_loss\n",
    "            \n",
    "        avg_actor_loss = total_actor_loss / mini_batch\n",
    "        avg_critic_loss = total_critic_loss / mini_batch\n",
    "        \n",
    "        writer.add_scalar('Actor loss', avg_actor_loss, epoch)\n",
    "        writer.add_scalar('Critic loss', avg_critic_loss, epoch)\n",
    "        \n",
    "        writer.flush()\n",
    "        \n",
    "        \n",
    "        print(f'epoch: {epoch} | avg actor loss: {avg_actor_loss:.3f} | avg critic loss: {avg_critic_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82997e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | avg actor loss: -11.518952599726617 | avg critic loss: 20.772311560809612\n",
      "epoch: 1 | avg actor loss: -16.364063054323196 | avg critic loss: 10.611696988344193\n",
      "epoch: 2 | avg actor loss: -18.82680258154869 | avg critic loss: 7.727895721793175\n",
      "epoch: 3 | avg actor loss: -22.1043761074543 | avg critic loss: 3.7576239332556725\n",
      "epoch: 4 | avg actor loss: -23.777189791202545 | avg critic loss: 2.1902577728033066\n",
      "epoch: 5 | avg actor loss: -25.13528737425804 | avg critic loss: 1.835844200104475\n",
      "epoch: 6 | avg actor loss: -26.913690745830536 | avg critic loss: 1.4302853904664516\n",
      "epoch: 7 | avg actor loss: -28.622118592262268 | avg critic loss: 1.1147063923999667\n",
      "epoch: 8 | avg actor loss: -29.77080574631691 | avg critic loss: 0.9762753564864397\n",
      "epoch: 9 | avg actor loss: -31.36554816365242 | avg critic loss: 0.8422545026987791\n"
     ]
    }
   ],
   "source": [
    "train_model(num_tasks = 10, epochs = 10, mini_batch = 64, batch_size = 256, fixed_length = 256)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
