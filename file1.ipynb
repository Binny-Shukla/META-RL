{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c84a4a9",
   "metadata": {},
   "source": [
    "# **REPTILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e0388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b1ebd",
   "metadata": {},
   "source": [
    "### **LOGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182df74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/Reptile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a6d23",
   "metadata": {},
   "source": [
    "### **DEVICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ef35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e17c1",
   "metadata": {},
   "source": [
    "### **META - ENV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42036578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_env:\n",
    "    \n",
    "    def __init__(self, env_name):\n",
    "        \n",
    "        self.base_env = gym.make(env_name)\n",
    "        \n",
    "    def sample_task(self, num_tasks):\n",
    "        \n",
    "        self.tasks = []\n",
    "        \n",
    "        for _ in range(num_tasks):\n",
    "            \n",
    "            goal_position = np.random.uniform(0.45, 0.55)\n",
    "            gravity = np.random.uniform(0.0025, 0.006)\n",
    "            \n",
    "            self.tasks.append((goal_position, gravity))\n",
    "            \n",
    "        return self.tasks\n",
    "    \n",
    "    def set_task(self, task):\n",
    "        \n",
    "        self.base_env.env.goal_position = task[0]\n",
    "        self.base_env.env.gravity = task[1]\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        obs = self.base_env.reset()\n",
    "        \n",
    "        if isinstance(obs, tuple):\n",
    "            \n",
    "            obs = obs[0]\n",
    "            \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action_np):\n",
    "        \n",
    "        next_state, reward, termination, timeout, info = self.base_env.step(action_np)\n",
    "        done = termination or timeout\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_number(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = self.base_env.action_space.high[0]\n",
    "        reward_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, reward_dim\n",
    "    \n",
    "    def close(self):\n",
    "        \n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbd51e",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_ENV = meta_env('MountainCarContinuous-v0')\n",
    "\n",
    "state_dim, action_dim, max_action, reward_dim = META_ENV.get_number()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ed963",
   "metadata": {},
   "source": [
    "### **HELPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b999348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e70db",
   "metadata": {},
   "source": [
    "### **ASSEMBLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 32\n",
    "head_2 = 64\n",
    "head_3 = 64\n",
    "head_4 = 32\n",
    "\n",
    "hidden_size = 32\n",
    "hidden_size_2 = 64\n",
    "hidden_size_3 = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7e6c7",
   "metadata": {},
   "source": [
    "### **HYPER X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4efa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_x(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, hidden_size = hidden_size, hidden_size_2 = hidden_size_2, hidden_size_3 = hidden_size_3):\n",
    "        super(hyper_x, self).__init__()\n",
    "        \n",
    "        # input dim\n",
    "        \n",
    "        input_dim = state_dim\n",
    "        \n",
    "        # hyper mlp\n",
    "        \n",
    "        self.hyper = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size_3),\n",
    "            nn.LayerNorm(hidden_size_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_3, hidden_size_3),\n",
    "            nn.LayerNorm(hidden_size_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_3, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "                \n",
    "    def forward(self, state):\n",
    "        \n",
    "        hyper = self.hyper(state)\n",
    "        \n",
    "        return hyper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a8825",
   "metadata": {},
   "source": [
    "### **POLICY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b46916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, action_dim = action_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action):\n",
    "        super(policy_net, self).__init__()\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # hyper\n",
    "        \n",
    "        self.hyper = hyper_x()\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        self.hyper_norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # post mlp\n",
    "        \n",
    "        self.post_process = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # mu and log std head\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        # add normalization\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        \n",
    "        if isinstance(m, nn.Linear):\n",
    "            \n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            \n",
    "            if m.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "    def forward(self, state, deterministic = False):\n",
    "        \n",
    "        # state -> hyper\n",
    "        \n",
    "        hyper = self.hyper(state)\n",
    "        \n",
    "        # hyper -> norm\n",
    "        \n",
    "        norm = self.hyper_norm(hyper)\n",
    "        \n",
    "        # norm -> post process\n",
    "        \n",
    "        post_process = self.post_process(norm)\n",
    "        \n",
    "        # mu and log std\n",
    "        \n",
    "        mu = self.mu(post_process)\n",
    "        \n",
    "        if deterministic:\n",
    "            \n",
    "            return mu\n",
    "        \n",
    "        log_std = self.log_std(post_process)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # reparameterization\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        z = dist.rsample()\n",
    "        tanh_z = torch.tanh(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        log_prob = dist.log_prob(z)\n",
    "        squash = torch.log(1 - tanh_z.pow(2) + 1e-6)\n",
    "        log_prob = log_prob - squash\n",
    "        \n",
    "        log_prob = log_prob.sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        entropy = dist.entropy().sum(dim = -1).mean()\n",
    "        \n",
    "        return action, log_prob, entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6796221",
   "metadata": {},
   "source": [
    "### **CRITIC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ddb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, action_dim = action_dim, state_dim = state_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(critic_net, self).__init__()\n",
    "        \n",
    "        # input dim\n",
    "        \n",
    "        input_dim = state_dim + action_dim\n",
    "        \n",
    "        # pre process \n",
    "        \n",
    "        self.pre_process = nn.Linear(input_dim, head_1)\n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # post process\n",
    "        \n",
    "        self.post_process = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        self.critic = nn.Linear(head_4, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        # cat\n",
    "        \n",
    "        cat = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        # pre process\n",
    "        \n",
    "        pre = self.pre_process(cat)\n",
    "        \n",
    "        # post \n",
    "        \n",
    "        post = self.post_process(pre)\n",
    "        \n",
    "        # critic\n",
    "        \n",
    "        q = self.critic(post)\n",
    "        \n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3744264",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_net(\n",
      "  (hyper): hyper_x(\n",
      "    (hyper): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "      (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): SiLU()\n",
      "      (12): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (13): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (14): SiLU()\n",
      "      (15): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (16): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (17): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (hyper_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_process): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (log_std): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "critic_net(\n",
      "  (pre_process): Linear(in_features=3, out_features=32, bias=True)\n",
      "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_process): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "  )\n",
      "  (critic): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# policy network\n",
    "\n",
    "POLICY_NET = policy_net().to(device)\n",
    "\n",
    "print(POLICY_NET)\n",
    "\n",
    "print('-' * 200)\n",
    "\n",
    "# critic network\n",
    "\n",
    "CRITIC_NET = critic_net().to(device)\n",
    "\n",
    "print(CRITIC_NET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169dbf64",
   "metadata": {},
   "source": [
    "### **OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c17528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "policy_lr = 1e-4\n",
    "\n",
    "critic_lr = 3e-4\n",
    "\n",
    "meta_lr = 0.005\n",
    "\n",
    "T_max = 10\n",
    "\n",
    "warmup_epochs = 5\n",
    "\n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW([\n",
    "    \n",
    "    {'params': CRITIC_NET.parameters(), 'lr': critic_lr, 'weight_decay': 1e-6},\n",
    "    {'params': POLICY_NET.parameters(), 'lr': policy_lr, 'weight_decay': 0}\n",
    "])\n",
    "\n",
    "\n",
    "# scheduler\n",
    "\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(OPTIMIZER, start_factor = 0.1, total_iters = warmup_epochs)\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = T_max - warmup_epochs, eta_min = 1e-5)\n",
    "\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.SequentialLR(OPTIMIZER, [warmup_scheduler, cosine_scheduler], milestones = [warmup_epochs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecd344",
   "metadata": {},
   "source": [
    "### **BUFFER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_buffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, state, action, log_prob, reward, done, next_state, entropy):\n",
    "        \n",
    "        # add\n",
    "        \n",
    "        self.buffer.append({\n",
    "            \n",
    "            'states': safe_tensor(state),\n",
    "            'actions': safe_tensor(action),\n",
    "            'log_probs': safe_tensor(log_prob),\n",
    "            'rewards': safe_tensor(reward),\n",
    "            'dones': safe_tensor(done),\n",
    "            'next_states': safe_tensor(next_state),\n",
    "            'entropy': safe_tensor(entropy)\n",
    "        })\n",
    "        \n",
    "    def safe_stack(self, x):\n",
    "        \n",
    "        return torch.stack(x).to(device)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        # stack\n",
    "        \n",
    "        states = [i['states'] for i in self.buffer]\n",
    "        actions = [i['actions'] for i in self.buffer]\n",
    "        log_probs = [i['log_probs'] for i in self.buffer]\n",
    "        rewards = [i['rewards'] for i in self.buffer]\n",
    "        dones = [i['dones'] for i in self.buffer]\n",
    "        next_states = [i['next_states'] for i in self.buffer]\n",
    "        entropy = [i['entropy'] for i in self.buffer]\n",
    "        \n",
    "        # batch\n",
    "        \n",
    "        batch = {\n",
    "            \n",
    "            'states': self.safe_stack(states),\n",
    "            'actions': self.safe_stack(actions),\n",
    "            'log_probs': self.safe_stack(log_probs),\n",
    "            'rewards': self.safe_stack(rewards),\n",
    "            'dones': self.safe_stack(dones),\n",
    "            'next_states': self.safe_stack(next_states),\n",
    "            'entropy': self.safe_stack(entropy)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def clear(self):\n",
    "        \n",
    "        self.buffer.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946276a4",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_BUFFER = meta_buffer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69daf73",
   "metadata": {},
   "source": [
    "### **COMPUTE LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    \n",
    "    def __init__(self, gamma, gae_lam, entropy_coef, POLICY_NET = POLICY_NET, CRITIC_NET = CRITIC_NET, META_BUFFER = META_BUFFER):\n",
    "        \n",
    "        # hyper param\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lam = gae_lam\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # network\n",
    "        \n",
    "        self.policy = POLICY_NET\n",
    "        self.critic = CRITIC_NET\n",
    "        \n",
    "        # buffer\n",
    "        \n",
    "        self.buffer = META_BUFFER\n",
    "\n",
    "    def compute_gae(self, rewards, dones, value, last_value):\n",
    "        \n",
    "        values = torch.cat([value, last_value], dim = 0)\n",
    "        \n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            \n",
    "            delta = rewards[step] + self.gamma * (1 - dones[step]) * values[step + 1] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lam * (1 - dones[step]) * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "            \n",
    "        advantages = safe_tensor(advantages)\n",
    "        \n",
    "        returns = [adv + val for adv, val in zip(advantages, value)]\n",
    "        \n",
    "        returns = safe_tensor(returns)\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def critic_loss(self, value, returns):\n",
    "        \n",
    "        value = value.squeeze(1)\n",
    "        \n",
    "        returns = returns.detach()\n",
    "        \n",
    "        critic_loss = F.mse_loss(value, returns)\n",
    "        \n",
    "        return critic_loss\n",
    "    \n",
    "    def policy_loss(self, log_probs, advantages, entropy):\n",
    "        \n",
    "        advantages = advantages.detach()\n",
    "        \n",
    "        policy_loss = (- log_probs * advantages).mean() - self.entropy_coef * entropy.mean()\n",
    "        \n",
    "        return policy_loss\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \n",
    "        # sample\n",
    "        \n",
    "        batch = self.buffer.sample()\n",
    "        \n",
    "        # unpack\n",
    "        \n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        log_probs = batch['log_probs']\n",
    "        rewards = batch['rewards']\n",
    "        dones = batch['dones']\n",
    "        next_states = batch['next_states']\n",
    "        entropy = batch['entropy']\n",
    "        \n",
    "        # shape check\n",
    "        \n",
    "        rewards = rewards.view(-1, 1)\n",
    "        dones = dones.view(-1, 1)\n",
    "        \n",
    "        # get value from critic through policy action\n",
    "        \n",
    "        value = self.critic.forward(states, actions)    \n",
    "        \n",
    "        # get last value from critic through next state\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            last_state = next_states[-1:]\n",
    "            \n",
    "            last_action, _, _ = self.policy.forward(last_state)\n",
    "            \n",
    "            last_value = self.critic.forward(last_state, last_action)\n",
    "            \n",
    "        # compute gae\n",
    "        \n",
    "        advantages, returns = self.compute_gae(rewards, dones, value, last_value)\n",
    "        \n",
    "        # compute critic loss\n",
    "        \n",
    "        critic_loss = self.critic_loss(value, returns)\n",
    "        \n",
    "        # compute policy loss\n",
    "        \n",
    "        policy_loss = self.policy_loss(log_probs, advantages, entropy)\n",
    "        \n",
    "        total_loss = policy_loss + critic_loss\n",
    "        \n",
    "        return total_loss, policy_loss, critic_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc6527",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper param\n",
    "\n",
    "gamma = 0.99\n",
    "gae_lam = 0.95\n",
    "entropy_coef = 0.01\n",
    "\n",
    "# setup\n",
    "\n",
    "LOSS_FUNCTION = loss_func(gamma, gae_lam, entropy_coef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a68c0e",
   "metadata": {},
   "source": [
    "### **HELPER 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ba912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_params(agent):\n",
    "    \n",
    "    return [p.clone().detach() for p in agent.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b8af2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_update(meta_params, adapted_params, meta_lr):\n",
    "    \n",
    "    for param, adapted in zip(meta_params, adapted_params):\n",
    "        \n",
    "        param.data = param.data + meta_lr * (adapted.data - param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a16bf",
   "metadata": {},
   "source": [
    "### **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b7ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAINING_LOOP(meta_iteration, inner_steps, batch_size, meta_lr, META_ENV = META_ENV, POLICY_NET = POLICY_NET, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER, LOSS_FUNCTION = LOSS_FUNCTION):\n",
    "    \n",
    "    for iteration in range(meta_iteration):\n",
    "        \n",
    "        meta_params = get_current_params(POLICY_NET)\n",
    "        \n",
    "        total_agent_loss, total_policy_loss, total_critic_loss = 0.0, 0.0, 0.0\n",
    "        \n",
    "        for task in META_ENV.sample_task(batch_size):\n",
    "        \n",
    "            META_ENV.set_task(task)\n",
    "            \n",
    "            obs = META_ENV.reset()\n",
    "            \n",
    "            obs = safe_tensor(obs).unsqueeze(0)\n",
    "            \n",
    "            META_BUFFER.clear()\n",
    "            \n",
    "            # inner loop\n",
    "            \n",
    "            for step in range(inner_steps):\n",
    "                \n",
    "                action, log_prob, entropy = POLICY_NET.forward(obs)\n",
    "                \n",
    "                action_np = action.detach().cpu().numpy()[0]\n",
    "                \n",
    "                next_state, reward, done, _ = META_ENV.step(action_np)\n",
    "                \n",
    "                next_state = safe_tensor(next_state).unsqueeze(0)\n",
    "                \n",
    "                META_BUFFER.add(obs.squeeze(0), action.squeeze(0), log_prob.squeeze(0), reward, done, next_state.squeeze(0), entropy)\n",
    "                \n",
    "                obs = next_state\n",
    "                \n",
    "                if done:\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                \n",
    "            # compute loss\n",
    "            \n",
    "            agent_loss, policy_loss, critic_loss = LOSS_FUNCTION.compute_loss()\n",
    "            \n",
    "            OPTIMIZER.zero_grad()\n",
    "            agent_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(POLICY_NET.parameters(), max_norm = 0.5)\n",
    "            torch.nn.utils.clip_grad_norm_(CRITIC_NET.parameters(), max_norm = 0.5)\n",
    "            OPTIMIZER.step()\n",
    "            SCHEDULER.step()\n",
    "            \n",
    "            total_agent_loss += agent_loss.item()\n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "            \n",
    "        avg_agent_loss = total_agent_loss / batch_size\n",
    "        avg_policy_loss = total_policy_loss / batch_size\n",
    "        avg_critic_loss = total_critic_loss / batch_size\n",
    "        \n",
    "        writer.add_scalar('Agent loss', avg_agent_loss, iteration)\n",
    "        writer.add_scalar('Policy loss', policy_loss, iteration)\n",
    "        writer.add_scalar('Critic loss', critic_loss, iteration)\n",
    "        \n",
    "        adapted_params = get_current_params(POLICY_NET)\n",
    "        meta_update(meta_params, adapted_params, meta_lr)\n",
    "        \n",
    "        print(f'epoch: {iteration} | agent loss: {avg_agent_loss:.3f} | policy loss: {avg_policy_loss:.3f} | critic loss: {avg_critic_loss:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd12b28",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04726f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | agent loss: 1.142 | policy loss: -0.016 | critic loss: 1.158\n",
      "epoch: 1 | agent loss: 0.933 | policy loss: -0.019 | critic loss: 0.952\n",
      "epoch: 2 | agent loss: 0.607 | policy loss: -0.024 | critic loss: 0.632\n",
      "epoch: 3 | agent loss: 0.311 | policy loss: -0.034 | critic loss: 0.345\n",
      "epoch: 4 | agent loss: 0.139 | policy loss: -0.034 | critic loss: 0.173\n",
      "epoch: 5 | agent loss: 0.026 | policy loss: -0.034 | critic loss: 0.060\n",
      "epoch: 6 | agent loss: -0.026 | policy loss: -0.034 | critic loss: 0.008\n",
      "epoch: 7 | agent loss: -0.030 | policy loss: -0.034 | critic loss: 0.004\n",
      "epoch: 8 | agent loss: -0.030 | policy loss: -0.034 | critic loss: 0.004\n",
      "epoch: 9 | agent loss: -0.030 | policy loss: -0.034 | critic loss: 0.004\n"
     ]
    }
   ],
   "source": [
    "# param\n",
    "\n",
    "meta_iteration = 10\n",
    "inner_steps = 64\n",
    "batch_size = 256\n",
    "\n",
    "TRAINING_LOOP(meta_iteration, inner_steps, batch_size, meta_lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
