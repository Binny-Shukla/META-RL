{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ac5602",
   "metadata": {},
   "source": [
    "# **R2D2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85933e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb5ca5",
   "metadata": {},
   "source": [
    "## **DEVICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b592089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d49b4",
   "metadata": {},
   "source": [
    "#### **LOGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546471fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/R2D2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81806985",
   "metadata": {},
   "source": [
    "### **HELPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075e3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c140b1f",
   "metadata": {},
   "source": [
    "### **META - ENV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a772287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_env:\n",
    "    \n",
    "    def __init__(self, env_name):\n",
    "        \n",
    "        self.base_env = gym.make(env_name)\n",
    "        \n",
    "    def sample_tasks(self, num_tasks):\n",
    "        \n",
    "        self.tasks = []\n",
    "        \n",
    "        for _ in range(num_tasks):\n",
    "            \n",
    "            gravity = np.random.uniform(5.0, 15.0)\n",
    "            torso_mass = np.random.uniform(1.0, 5.0)\n",
    "            target_velocity = np.random.uniform(0.5, 3.0)\n",
    "            \n",
    "            self.tasks.append((gravity, torso_mass, target_velocity))\n",
    "            \n",
    "        return self.tasks\n",
    "    \n",
    "    def set_task(self, task):\n",
    "        \n",
    "        self.base_env.unwrapped.model.opt.gravity[-1] = -task[0]\n",
    "        self.base_env.unwrapped.model.body_mass[1] = task[1]\n",
    "        self.target_velocity = task[2]\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        obs = self.base_env.reset()\n",
    "        \n",
    "        if isinstance(obs, tuple):\n",
    "            \n",
    "            obs = obs[0]\n",
    "            \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        next_obs, reward, termination, timeout, info = self.base_env.step(action)\n",
    "        \n",
    "        vel = self.base_env.unwrapped.data.qvel[0]\n",
    "        reward -= 0.5 * abs(vel - self.target_velocity)\n",
    "        \n",
    "        done = termination or timeout\n",
    "        \n",
    "        return next_obs, reward, done, info\n",
    "    \n",
    "    def get_number(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = self.base_env.action_space.high[0]\n",
    "        \n",
    "        reward_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, reward_dim\n",
    "    \n",
    "    def close(self):\n",
    "        \n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c434d",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b4c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: (17,)\n",
      "\n",
      "state dim: 17 | action dim: 6 | max action: 1.0\n",
      "\n",
      "Tasks: [(6.520299342867645, 1.729326276002701, 0.9123206413768365), (13.534885322830993, 1.5328676850417442, 0.941549114309512), (5.750121944926288, 2.1808702862732456, 1.6562976715008144)]\n"
     ]
    }
   ],
   "source": [
    "META_ENV = meta_env('Walker2d-v5')\n",
    "\n",
    "tasks = META_ENV.sample_tasks(3)\n",
    "\n",
    "for task in tasks:\n",
    "    \n",
    "    META_ENV.set_task(task)\n",
    "    obs = META_ENV.reset()\n",
    "    \n",
    "state_dim, action_dim, max_action, reward_dim = META_ENV.get_number()\n",
    "    \n",
    "print(f'obs: {obs.shape}')\n",
    "print()\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action}')  \n",
    "print()\n",
    "print(f'Tasks: {tasks}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f855e",
   "metadata": {},
   "source": [
    "### **ASSEMBLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68dd0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "head_1 = 128\n",
    "head_2 = 256\n",
    "head_3 = 256\n",
    "head_4 = 128\n",
    "\n",
    "hidden_size = 64\n",
    "hidden_size_2 = 128\n",
    "hidden_size_3 = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d373a85",
   "metadata": {},
   "source": [
    "### **HYPER X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9cccd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_x(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, hidden_size = hidden_size, hidden_size_2 = hidden_size_2):\n",
    "        super(hyper_x, self).__init__()\n",
    "        \n",
    "        # input dim\n",
    "        \n",
    "        input_dim = state_dim\n",
    "        \n",
    "        # hyper x\n",
    "        \n",
    "        self.hyper = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size_3),\n",
    "            nn.LayerNorm(hidden_size_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(hidden_size_3, hidden_size_3),\n",
    "            nn.LayerNorm(hidden_size_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_3, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        hyper = self.hyper(state)\n",
    "        \n",
    "        return hyper\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03506f3f",
   "metadata": {},
   "source": [
    "### **R2D2 DESIGN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e53d9de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class r2d2(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action, action_dim = action_dim, hidden_size_2 = hidden_size_2):\n",
    "        super(r2d2, self).__init__()\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # critic input\n",
    "        \n",
    "        critic_input = state_dim\n",
    "        \n",
    "        self.critic_pre_process = nn.Linear(critic_input, hidden_size_2)\n",
    "        self.critic_pre_norm = nn.LayerNorm(hidden_size_2)\n",
    "        \n",
    "        # hyper x\n",
    "        \n",
    "        self.hyper = hyper_x()\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        self.norm = nn.LayerNorm(hidden_size_2)\n",
    "        \n",
    "        # process more\n",
    "        \n",
    "        self.process = nn.Linear(hidden_size_2, head_1)\n",
    "        self.process_norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # actor lstm\n",
    "        \n",
    "        self.actor_lstm = nn.LSTM(head_1, head_1, num_layers = 2, batch_first = True)\n",
    "        \n",
    "        # critic lstm\n",
    "        \n",
    "        self.critic_lstm = nn.LSTM(head_1, head_1, num_layers = 2, batch_first = True)\n",
    "        \n",
    "        # post norm \n",
    "        \n",
    "        self.post_lstm_actor_norm = nn.LayerNorm(head_1)\n",
    "        self.post_lstm_critic_norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # actor mlp \n",
    "        \n",
    "        self.actor_mlp = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # critic mlp\n",
    "        \n",
    "        self.critic_mlp = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # actor mu and log std head\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        self.critic_head = nn.Linear(head_4, 1)\n",
    "        \n",
    "        # normalization\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        \n",
    "        if isinstance(m, nn.Linear):\n",
    "            \n",
    "            nn.init.orthogonal_(m.weight)\n",
    "            \n",
    "            if m.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "        elif isinstance(m, nn.LSTM):\n",
    "                \n",
    "            for name, param in m.named_parameters():\n",
    "                    \n",
    "                if 'weight_ih' in name:\n",
    "                        \n",
    "                    nn.init.kaiming_uniform_(param, math.sqrt(5))\n",
    "                        \n",
    "                elif 'weight_hh' in name:\n",
    "                        \n",
    "                    nn.init.orthogonal_(param)\n",
    "                        \n",
    "                elif 'bias' in name:\n",
    "                        \n",
    "                    nn.init.zeros_(param)\n",
    "                    \n",
    "                    n = param.size(0)\n",
    "                    start, end = n // 4, n // 2\n",
    "                    param.data[start:end].fill_(1.0)\n",
    "                    \n",
    "    def actor_forward(self, state, actor_hidden_memory = None):\n",
    "        \n",
    "        # state -> hyper\n",
    "        \n",
    "        hyper = self.hyper(state)\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        hyper_norm = self.norm(hyper)\n",
    "        \n",
    "        # process\n",
    "        \n",
    "        process = self.process(hyper_norm)\n",
    "        \n",
    "        # process norm\n",
    "        \n",
    "        process_norm = self.process_norm(process)\n",
    "        \n",
    "        # lstm input\n",
    "        \n",
    "        if process_norm.dim() == 2:\n",
    "            \n",
    "            process_norm = process_norm.unsqueeze(1)\n",
    "        \n",
    "            actor_lstm_out, h_a = self.actor_lstm(process_norm, actor_hidden_memory)\n",
    "            \n",
    "            actor_lstm_out = actor_lstm_out.squeeze(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            actor_lstm_out, h_a = self.actor_lstm(process_norm, actor_hidden_memory)\n",
    "            \n",
    "        # post norm\n",
    "        \n",
    "        post_lstm_norm = self.post_lstm_actor_norm(actor_lstm_out)\n",
    "            \n",
    "        # actor mlp\n",
    "        \n",
    "        actor_mlp = self.actor_mlp(post_lstm_norm)\n",
    "        \n",
    "        # mu and log head\n",
    "        \n",
    "        mu = self.mu(actor_mlp)\n",
    "        log_std = self.log_std(actor_mlp)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # action\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        z = dist.rsample()\n",
    "        tanh_z = torch.tanh(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        # log prob\n",
    "        \n",
    "        log_prob = dist.log_prob(z)\n",
    "        squash = torch.log(1 - tanh_z.pow(2) + 1e-6)\n",
    "        log_prob = log_prob - squash\n",
    "        log_prob = log_prob.sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        # entropy\n",
    "        \n",
    "        entropy = dist.entropy().sum(dim = -1)\n",
    "        \n",
    "        return action, log_prob, entropy, h_a\n",
    "    \n",
    "    def critic_forward(self, state, critic_hidden_memory):\n",
    "        \n",
    "        # pre process\n",
    "        \n",
    "        pre_process = self.critic_pre_process(state)\n",
    "        \n",
    "        # pre norm\n",
    "        \n",
    "        pre_norm = self.critic_pre_norm(pre_process)\n",
    "        \n",
    "        # critic lstm\n",
    "        \n",
    "        if pre_norm.dim() == 2: \n",
    "            \n",
    "            pre_norm = pre_norm.unsqueeze(1)\n",
    "\n",
    "            critic_lstm_out, h_c = self.critic_lstm(pre_norm, critic_hidden_memory)\n",
    "            \n",
    "            critic_lstm_out = critic_lstm_out.squeeze(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            critic_lstm_out, h_c = self.critic_lstm(pre_norm, critic_hidden_memory)\n",
    "            \n",
    "        # critic post norm\n",
    "        \n",
    "        critic_post_norm = self.post_lstm_critic_norm(critic_lstm_out)\n",
    "        \n",
    "        # critic mlp\n",
    "        \n",
    "        critic_mlp = self.critic_mlp(critic_post_norm)\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        critic_head = self.critic_head(critic_mlp)\n",
    "        \n",
    "        return critic_head, h_c   \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        h_a = torch.zeros(self.actor_lstm.num_layers, 1, self.actor_lstm.hidden_size).to(next(self.parameters()))\n",
    "        c_a = torch.zeros(self.actor_lstm.num_layers, 1, self.actor_lstm.hidden_size).to(next(self.parameters()))\n",
    "        \n",
    "        h_c = torch.zeros(self.actor_lstm.num_layers, 1, self.critic_lstm.hidden_size).to(next(self.parameters()))\n",
    "        c_c = torch.zeros(self.actor_lstm.num_layers, 1, self.critic_lstm.hidden_size).to(next(self.parameters()))\n",
    "        \n",
    "        return (h_a, c_a), (h_c, c_c)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc11858",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758db1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2d2(\n",
      "  (critic_pre_process): Linear(in_features=17, out_features=128, bias=True)\n",
      "  (critic_pre_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (hyper): hyper_x(\n",
      "    (hyper): Sequential(\n",
      "      (0): Linear(in_features=17, out_features=64, bias=True)\n",
      "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (10): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): SiLU()\n",
      "      (12): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (13): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (14): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (process): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (process_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (actor_lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
      "  (critic_lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
      "  (post_lstm_actor_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (post_lstm_critic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (actor_mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (7): SiLU()\n",
      "  )\n",
      "  (critic_mlp): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (7): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (log_std): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (critic_head): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# r2d2\n",
    "\n",
    "R2D2_NETWORK = r2d2().to(device)\n",
    "\n",
    "print(R2D2_NETWORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27bccf",
   "metadata": {},
   "source": [
    "### **OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455609c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "hyper_lr = 1e-5\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 3e-4\n",
    "\n",
    "# T max\n",
    "\n",
    "warmup = 5\n",
    "T_max = 10\n",
    "\n",
    "# param\n",
    "\n",
    "actor_param = list(R2D2_NETWORK.norm.parameters()) + \\\n",
    "              list(R2D2_NETWORK.process.parameters()) + \\\n",
    "              list(R2D2_NETWORK.process_norm.parameters()) + \\\n",
    "              list(R2D2_NETWORK.actor_lstm.parameters()) + \\\n",
    "              list(R2D2_NETWORK.post_lstm_actor_norm.parameters()) + \\\n",
    "              list(R2D2_NETWORK.actor_mlp.parameters()) + \\\n",
    "              list(R2D2_NETWORK.mu.parameters()) + \\\n",
    "              list(R2D2_NETWORK.log_std.parameters()) \n",
    "              \n",
    "hyper_param = list(R2D2_NETWORK.hyper.parameters())\n",
    "\n",
    "critic_param = list(R2D2_NETWORK.critic_pre_process.parameters()) + \\\n",
    "               list(R2D2_NETWORK.critic_pre_norm.parameters()) + \\\n",
    "               list(R2D2_NETWORK.critic_lstm.parameters()) + \\\n",
    "               list(R2D2_NETWORK.post_lstm_critic_norm.parameters()) + \\\n",
    "               list(R2D2_NETWORK.critic_mlp.parameters()) + \\\n",
    "               list(R2D2_NETWORK.critic_head.parameters())   \n",
    "\n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW([\n",
    "    \n",
    "    {'params': critic_param, 'lr': critic_lr, 'weight_decay': 1e-6},\n",
    "    {'params': hyper_param, 'lr': hyper_lr, 'weight_decay': 0},\n",
    "    {'params': actor_param, 'lr': actor_lr, 'weight_decay': 0}\n",
    "])\n",
    "\n",
    "# scheduler\n",
    "\n",
    "warmup_sch = optim.lr_scheduler.LinearLR(OPTIMIZER, 0.1, total_iters = warmup)\n",
    "cosine_sch = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = T_max - warmup, eta_min = 1e-6)\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.SequentialLR(OPTIMIZER, [warmup_sch, cosine_sch], [warmup])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af854a",
   "metadata": {},
   "source": [
    "### **BUFFER**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd76b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_buffer:\n",
    "    \n",
    "    def __init__(self, max_episodes):\n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.episodes = []\n",
    "        self.current_episode = []\n",
    "        \n",
    "    def add(self, state, action, log_prob, reward, done, next_state, actor_memory, critic_memory):\n",
    "        \n",
    "        # actor memory\n",
    "        \n",
    "        h_a = actor_memory[0]\n",
    "        c_a = actor_memory[1]\n",
    "        \n",
    "        # critic memory\n",
    "        \n",
    "        h_c = critic_memory[0]\n",
    "        c_c = critic_memory[1]\n",
    "        \n",
    "        # add to current episode\n",
    "        \n",
    "        step = {\n",
    "            \n",
    "            'states': safe_tensor(state),\n",
    "            'actions': safe_tensor(action),\n",
    "            'log_probs': safe_tensor(log_prob),\n",
    "            'rewards': safe_tensor(reward),\n",
    "            'dones': safe_tensor(done),\n",
    "            'next_states': safe_tensor(next_state),\n",
    "            'h_a': h_a,\n",
    "            'c_a': c_a,\n",
    "            'h_c': h_c,\n",
    "            'c_c': c_c\n",
    "            \n",
    "        }\n",
    "        \n",
    "        self.current_episode.append(step)\n",
    "        \n",
    "        if safe_tensor(done).item() == 1:\n",
    "            \n",
    "            self.episodes.append(self.current_episode)\n",
    "            self.current_episode = []\n",
    "            \n",
    "        if len(self.episodes) > self.max_episodes:\n",
    "                \n",
    "            self.episodes.pop(0)\n",
    "                \n",
    "                \n",
    "    def sample(self, batch_size, unroll_len, burn_in):\n",
    "        \n",
    "        sampled_ep = random.sample(self.episodes, k = min(batch_size, len(self.episodes)))\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        for ep in sampled_ep:\n",
    "            \n",
    "            if len(ep) < unroll_len + burn_in:\n",
    "                \n",
    "                seg = self.padding(ep, burn_in, unroll_len)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                start_idx = random.randint(0, len(ep) - (unroll_len + burn_in))\n",
    "                \n",
    "                seg = ep[start_idx : start_idx + unroll_len + burn_in]\n",
    "            \n",
    "            segments.append(seg)\n",
    "            \n",
    "        \n",
    "        def safe_stack(x, seq):\n",
    "            \n",
    "            return torch.stack([torch.stack([s[x] for s in seg]) for seg in seq]).to(device)\n",
    "        \n",
    "        burn_in_batch = {\n",
    "            \n",
    "            'states': safe_stack('states', [seg[:burn_in] for seg in segments]),\n",
    "            'actions': safe_stack('actions', [seg[:burn_in] for seg in segments]),\n",
    "            'log_probs': safe_stack('log_probs', [seg[:burn_in] for seg in segments]),\n",
    "            'rewards': safe_stack('rewards', [seg[:burn_in] for seg in segments]),\n",
    "            'dones': safe_stack('dones', [seg[:burn_in] for seg in segments]),\n",
    "            'next_states': safe_stack('next_states', [seg[:burn_in] for seg in segments]),\n",
    "\n",
    "            'h_a': torch.stack([seg[0]['h_a'] for seg in segments]).to(device),\n",
    "            'c_a': torch.stack([seg[0]['c_a'] for seg in segments]).to(device),\n",
    "            'h_c': torch.stack([seg[0]['h_c'] for seg in segments]).to(device),\n",
    "            'c_c': torch.stack([seg[0]['c_c'] for seg in segments]).to(device)\n",
    "            \n",
    "        }\n",
    "\n",
    "        # training portion (after burn-in)\n",
    "        \n",
    "        train_batch = {\n",
    "            \n",
    "            'states': safe_stack('states', [seg[burn_in:] for seg in segments]),\n",
    "            'actions': safe_stack('actions', [seg[burn_in:] for seg in segments]),\n",
    "            'log_probs': safe_stack('log_probs', [seg[burn_in:] for seg in segments]),\n",
    "            'rewards': safe_stack('rewards', [seg[burn_in:] for seg in segments]),\n",
    "            'dones': safe_stack('dones', [seg[burn_in:] for seg in segments]),\n",
    "            'next_states': safe_stack('next_states', [seg[burn_in:] for seg in segments]),\n",
    "\n",
    "            # hidden states here are NOT reset — burn-in already set them up\n",
    "            'h_a': torch.stack([seg[burn_in]['h_a'] for seg in segments]).to(device),\n",
    "            'c_a': torch.stack([seg[burn_in]['c_a'] for seg in segments]).to(device),\n",
    "            'h_c': torch.stack([seg[burn_in]['h_c'] for seg in segments]).to(device),\n",
    "            'c_c': torch.stack([seg[burn_in]['c_c'] for seg in segments]).to(device)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        return burn_in_batch, train_batch\n",
    "        \n",
    "    def padding(self, ep, burn_in, unroll_len):\n",
    "        \n",
    "        pad_length = (burn_in + unroll_len) - len(ep) \n",
    "        \n",
    "        last_step = ep[-1]\n",
    "        \n",
    "        pad_step = {}\n",
    "        \n",
    "        for k, v in last_step.items():\n",
    "            \n",
    "            if torch.is_tensor(v):\n",
    "                \n",
    "                pad_step[k] = v\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                pad_step[k] = v.clone()\n",
    "                \n",
    "        return ep + [pad_step] * pad_length\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5bbb0",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b120925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max ep\n",
    "\n",
    "max_episodes = 500\n",
    "\n",
    "# setup\n",
    "\n",
    "META_BUFFER = meta_buffer(max_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c6599",
   "metadata": {},
   "source": [
    "### **META RUNNER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac21361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_runner:\n",
    "    \n",
    "    def __init__(self, max_episode_range, R2D2_NETWORK = R2D2_NETWORK, META_BUFFER = META_BUFFER, META_ENV = META_ENV):\n",
    "        \n",
    "        self.network = R2D2_NETWORK\n",
    "        self.buffer = META_BUFFER\n",
    "        self.env = META_ENV\n",
    "        self.max_episode_range = max_episode_range\n",
    "        \n",
    "    def run(self, num_tasks):\n",
    "        \n",
    "        tasks = self.env.sample_tasks(num_tasks)\n",
    "        \n",
    "        for task in tasks:\n",
    "            \n",
    "            self.env.set_task(task)\n",
    "            \n",
    "            obs = self.env.reset()\n",
    "            \n",
    "            obs = safe_tensor(obs).unsqueeze(0)\n",
    "            \n",
    "            hidden_actor_memory, hidden_critic_memory = self.network.init_hidden()\n",
    "            \n",
    "            for _ in range(self.max_episode_range):\n",
    "                \n",
    "                action, log_prob, _, actor_memory = self.network.actor_forward(obs, hidden_actor_memory)\n",
    "                \n",
    "                _, critic_memory = self.network.critic_forward(obs, hidden_critic_memory)\n",
    "\n",
    "                action_np = action.detach().cpu().numpy()[0]\n",
    "\n",
    "                next_obs, reward, done, _ = self.env.step(action_np)\n",
    "                \n",
    "                next_obs = safe_tensor(next_obs).unsqueeze(0)\n",
    "                \n",
    "                self.buffer.add(obs.squeeze(0), action.squeeze(0), log_prob.squeeze(0), [reward], done, next_obs.squeeze(0), hidden_actor_memory, hidden_critic_memory)\n",
    "                \n",
    "                obs = next_obs\n",
    "                hidden_actor_memory = actor_memory\n",
    "                hidden_critic_memory = critic_memory\n",
    "                \n",
    "                if done:\n",
    "                    \n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103017cd",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277451ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta runner\n",
    "\n",
    "max_episode_range = 512\n",
    "\n",
    "# setup\n",
    "\n",
    "META_RUNNER = meta_runner(max_episode_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252559e",
   "metadata": {},
   "source": [
    "### **LOSS FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15468fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    \n",
    "    def __init__(self, gamma, gae_lam, entropy_ceof, value_coef, clip_epsilon, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER, R2D2_NETWORK = R2D2_NETWORK):\n",
    "        \n",
    "        # network\n",
    "        \n",
    "        self.r2d2 = R2D2_NETWORK\n",
    "        \n",
    "        # hyper params\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lam = gae_lam\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_ceof\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        \n",
    "        # optimizer\n",
    "        \n",
    "        self.optimizer = OPTIMIZER\n",
    "        self.scheduler = SCHEDULER\n",
    "        \n",
    "        # buffer\n",
    "        \n",
    "        self.buffer = META_BUFFER\n",
    "        \n",
    "    def compute_gae(self, rewards, dones, value, last_value):\n",
    "        \n",
    "        values = torch.cat([value, last_value], dim = 1)\n",
    "        \n",
    "        gae = 0\n",
    "        advantages = []\n",
    "        \n",
    "        for step in reversed(range(rewards.size(1))):\n",
    "            \n",
    "            delta = rewards[:, step] + self.gamma * (1 - dones[:, step]) * values[:, step + 1] - values[:, step]\n",
    "            gae = delta + self.gae_lam * (1 - dones[:, step]) * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = torch.stack(advantages, dim = 1).to(device)\n",
    "        \n",
    "        returns = advantages + value\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n",
    "        \n",
    "        returns = safe_tensor(returns)\n",
    "    \n",
    "        return advantages.detach(), returns.detach()\n",
    "    \n",
    "    def policy_loss(self, old_log_probs, log_probs, advantages, entropy):\n",
    "        \n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        \n",
    "        surr1 = ratio * advantages    \n",
    "        \n",
    "        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        \n",
    "        surrogate_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        policy_loss = surrogate_loss - self.entropy_coef * entropy.mean()\n",
    "        \n",
    "        return policy_loss\n",
    "    \n",
    "    def value_loss(self, value, returns):\n",
    "        \n",
    "        v_loss = F.mse_loss(value, returns)\n",
    "        \n",
    "        value_loss = v_loss * self.value_coef\n",
    "        \n",
    "        return value_loss\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \n",
    "        # unpack batch\n",
    "        \n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        old_log_probs = batch['log_probs']\n",
    "        rewards = batch['rewards']\n",
    "        dones = batch['dones']\n",
    "        next_states = batch['next_states']\n",
    "        \n",
    "        h_a = batch['h_a']\n",
    "        c_a = batch['c_a']\n",
    "        h_c = batch['h_c']\n",
    "        c_c = batch['c_c']\n",
    "        \n",
    "        # shape correction\n",
    "        \n",
    "        last_state = next_states[:, -1:]\n",
    "        \n",
    "        h_a, c_a = h_a.squeeze(2), c_a.squeeze(2)\n",
    "        \n",
    "        c_c, h_c = c_c.squeeze(2), h_c.squeeze(2)\n",
    "        \n",
    "        dones = dones.unsqueeze(2)\n",
    "        \n",
    "        # memory allocation\n",
    "        \n",
    "        actor_memory = (h_a, c_a)\n",
    "        critic_memory = (h_c, c_c)\n",
    "        \n",
    "        # compute log probs and value\n",
    "        \n",
    "        _, log_probs, entropy, _ = self.r2d2.actor_forward(states, actor_memory)\n",
    "        \n",
    "        value, _ = self.r2d2.critic_forward(states, critic_memory)\n",
    "        \n",
    "        # compute last value\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            last_value, _ = self.r2d2.critic_forward(last_state, critic_memory)\n",
    "                    \n",
    "        # compute gae\n",
    "        \n",
    "        advantages, returns = self.compute_gae(rewards, dones, value, last_value)\n",
    "        \n",
    "        # policy loss\n",
    "        \n",
    "        policy_loss = self.policy_loss(old_log_probs.detach(), log_probs, advantages, entropy)\n",
    "        \n",
    "        # value loss \n",
    "        \n",
    "        value_loss = self.value_loss(value, returns)\n",
    "        \n",
    "        # lets total loss and update optimize\n",
    "        \n",
    "        Agent_loss = policy_loss + value_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        Agent_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.r2d2.parameters(), max_norm = 0.5)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        al = Agent_loss.detach().item()\n",
    "        vl = value_loss.detach().item()\n",
    "        pl = policy_loss.detach().item()\n",
    "        \n",
    "        return al, vl, pl\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f56e3",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97fe57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper param\n",
    "\n",
    "value_coef = 0.5\n",
    "clip_epsilon = 0.2\n",
    "entropy_coef = 0.01\n",
    "gamma = 0.99\n",
    "gae_lam = 0.95\n",
    "\n",
    "# setup\n",
    "\n",
    "LOSS_FUNCTION = loss_func(gamma, gae_lam, entropy_coef, value_coef, clip_epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b4890",
   "metadata": {},
   "source": [
    "### **R2D2 LOGIC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e37f7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2D2_LOGIC(burn_in_batch, NETWORK, train_batch):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Actor burn-in\n",
    "        \n",
    "        h_a, c_a = burn_in_batch['h_a'], burn_in_batch['c_a']\n",
    "        \n",
    "        h_a, c_a = h_a.squeeze(2), c_a.squeeze(2)\n",
    "        \n",
    "        h_a = h_a.permute(1, 0, 2).contiguous()\n",
    "        c_a = c_a.permute(1, 0, 2).contiguous()\n",
    "        \n",
    "        for t in range(burn_in_batch['states'].size(1)): \n",
    "            \n",
    "            _, _, _, (h_a, c_a) = NETWORK.actor_forward(\n",
    "                burn_in_batch['states'][:, t],\n",
    "                (h_a, c_a)\n",
    "            )\n",
    "        \n",
    "        # Critic burn-in\n",
    "        \n",
    "        h_c, c_c = burn_in_batch['h_c'], burn_in_batch['c_c']\n",
    "        \n",
    "        c_c, h_c = c_c.squeeze(2), h_c.squeeze(2)\n",
    "        \n",
    "        h_c = h_c.permute(1, 0, 2).contiguous()\n",
    "        c_c = c_c.permute(1, 0, 2).contiguous()\n",
    "        \n",
    "        for t in range(burn_in_batch['states'].size(1)):\n",
    "            \n",
    "            _, (h_c, c_c) = NETWORK.critic_forward(\n",
    "                burn_in_batch['states'][:, t],\n",
    "                (h_c, c_c)\n",
    "            )\n",
    "\n",
    "    # 2. Build the training batch using updated hidden states\n",
    "    \n",
    "    batch = {\n",
    "        \n",
    "        'states': train_batch['states'],\n",
    "        'actions': train_batch['actions'],\n",
    "        'log_probs': train_batch['log_probs'],\n",
    "        'rewards': train_batch['rewards'],\n",
    "        'dones': train_batch['dones'],\n",
    "        'next_states': train_batch['next_states'],\n",
    "        'h_a': h_a,  # from post-burn-in\n",
    "        'c_a': c_a,\n",
    "        'h_c': h_c,\n",
    "        'c_c': c_c\n",
    "    }\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be16065",
   "metadata": {},
   "source": [
    "### **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8a0cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(meta_iteration, mini_steps, num_tasks, burn_in, unroll_len, batch_size, R2D2_NETWORK = R2D2_NETWORK, LOSS_FUNCTION = LOSS_FUNCTION, META_BUFFER = META_BUFFER, META_RUNNER = META_RUNNER):\n",
    "    \n",
    "    for iteration in range(meta_iteration):\n",
    "        \n",
    "        META_RUNNER.run(num_tasks)\n",
    "        \n",
    "        total_agent_loss, total_policy_loss, total_value_loss = 0.0, 0.0, 0.0\n",
    "        \n",
    "        for _ in range(mini_steps):\n",
    "            \n",
    "            burn_in_batch, train_batch = META_BUFFER.sample(batch_size, unroll_len, burn_in)\n",
    "            \n",
    "            batch = R2D2_LOGIC(burn_in_batch, R2D2_NETWORK, train_batch)\n",
    "            \n",
    "            agent_loss, policy_loss, value_loss = LOSS_FUNCTION.update(batch)\n",
    "            \n",
    "            total_agent_loss += agent_loss\n",
    "            total_policy_loss += policy_loss\n",
    "            total_value_loss += value_loss\n",
    "            \n",
    "        avg_agent_loss = total_agent_loss / mini_steps\n",
    "        avg_policy_loss = total_policy_loss / mini_steps\n",
    "        avg_value_loss = total_value_loss / mini_steps\n",
    "        \n",
    "        writer.add_scalar('Agent loss', avg_agent_loss, iteration)\n",
    "        writer.add_scalar('Policy loss', avg_policy_loss, iteration)\n",
    "        writer.add_scalar('Value loss', avg_value_loss, iteration)\n",
    "        \n",
    "        writer.flush()\n",
    "        \n",
    "        print(f'Epoch: {iteration} | agent loss: {avg_agent_loss:.3f} | policy loss: {avg_policy_loss:.3f} | value loss: {avg_value_loss:.3f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9a56f",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05ce131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | agent loss: 15.449 | policy loss: 0.644 | value loss: 14.805\n",
      "Epoch: 1 | agent loss: 0.728 | policy loss: 0.115 | value loss: 0.613\n",
      "Epoch: 2 | agent loss: 0.467 | policy loss: 0.206 | value loss: 0.261\n",
      "Epoch: 3 | agent loss: 0.177 | policy loss: 0.032 | value loss: 0.144\n",
      "Epoch: 4 | agent loss: 0.110 | policy loss: 0.026 | value loss: 0.083\n",
      "Epoch: 5 | agent loss: 0.101 | policy loss: 0.031 | value loss: 0.070\n",
      "Epoch: 6 | agent loss: 0.111 | policy loss: 0.015 | value loss: 0.095\n",
      "Epoch: 7 | agent loss: 0.108 | policy loss: 0.006 | value loss: 0.102\n",
      "Epoch: 8 | agent loss: 0.150 | policy loss: 0.012 | value loss: 0.138\n",
      "Epoch: 9 | agent loss: 0.114 | policy loss: 0.007 | value loss: 0.107\n"
     ]
    }
   ],
   "source": [
    "meta_iteration = 10\n",
    "mini_steps = 64\n",
    "num_tasks = 10\n",
    "batch_size = 256            # why 512 cause max ep range is 512\n",
    "burn_in = 64\n",
    "unroll_len = 64\n",
    "\n",
    "train_loop(meta_iteration, mini_steps, num_tasks, burn_in, unroll_len, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
