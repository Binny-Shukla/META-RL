{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7bed75",
   "metadata": {},
   "source": [
    "# Contextual Cross Attention Meta-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9411a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d94edb",
   "metadata": {},
   "source": [
    "### L O G G I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe968b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/CTX')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4829b20",
   "metadata": {},
   "source": [
    "### **PREFERANCES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91737609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb94ff",
   "metadata": {},
   "source": [
    "### D E V I C E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08bae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device : {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c281b",
   "metadata": {},
   "source": [
    "### M E T A - E N V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f5b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta_env:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.base_env = gym.make('Walker2d-v5')\n",
    "        self.target_velocity = 1.0\n",
    "        \n",
    "    def sample_tasks(self, num_tasks):\n",
    "        \n",
    "        self.tasks = []\n",
    "        \n",
    "        for _ in range(num_tasks):\n",
    "            \n",
    "            gravity = np.random.uniform(5.0, 15.0)\n",
    "            torso_mass = np.random.uniform(1.0, 5.0)\n",
    "            target_velocity = np.random.uniform(0.5, 3.0)\n",
    "            \n",
    "            self.tasks.append((gravity, torso_mass, target_velocity))\n",
    "        \n",
    "        return self.tasks\n",
    "    \n",
    "    def set_task(self, task):\n",
    "        \n",
    "        raw_env = self.base_env.unwrapped\n",
    "        raw_env.model.opt.gravity[-1] = -task[0]\n",
    "        raw_env.model.body_mass[1] = task[1]\n",
    "        self.target_velocity = task[2]\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        obs, _ = self.base_env.reset()\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        next_obs, reward, termination, timeouts, info = self.base_env.step(action)\n",
    "\n",
    "        done = termination | timeouts\n",
    "        raw_env = self.base_env.unwrapped\n",
    "        vel = raw_env.data.qvel[0]\n",
    "        reward -= 0.5 * abs(vel - self.target_velocity)\n",
    "        \n",
    "        return next_obs, reward, done\n",
    "    \n",
    "    def get_number(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = self.base_env.action_space.high[0]\n",
    "        reward_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, reward_dim\n",
    "    \n",
    "    def close(self):\n",
    "        \n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17669754",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daba9823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: (17,)\n",
      "state dim: 17 | action dim: 6 | max action: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = Meta_env()\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "state_dim, action_dim, max_action, reward_dim = env.get_number()\n",
    "\n",
    "print(f'state shape: {obs.shape}')\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa76a9",
   "metadata": {},
   "source": [
    "### A S S E M B L Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc0e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 64\n",
    "head_2 = 128\n",
    "head_3 = 128\n",
    "head_4 = 64\n",
    "\n",
    "embed_dim = 64\n",
    "\n",
    "hidden_size = 128\n",
    "hidden_size_2 = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bad7ca",
   "metadata": {},
   "source": [
    "### H E L P E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d7432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f3036",
   "metadata": {},
   "source": [
    "### C O N T E X T - E N C O D E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc959fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class context_encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, state_dim = state_dim, action_dim = action_dim, reward_dim = reward_dim, embed_dim = embed_dim):\n",
    "        super(context_encoder, self).__init__()\n",
    "        \n",
    "        input_dim = 2 * state_dim + action_dim + reward_dim\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, head_1),\n",
    "            nn.LayerNorm(head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(head_4, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, state, action, reward, next_state):\n",
    "        \n",
    "        cat = torch.cat([state, action, reward, next_state], dim = -1)\n",
    "        \n",
    "        encode = self.encode(cat)\n",
    "    \n",
    "        out = F.silu(self.norm(self.out(encode)))\n",
    "        \n",
    "        return out        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815a819",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f159b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_encoder(\n",
      "  (encode): Sequential(\n",
      "    (0): Linear(in_features=41, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (10): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (11): SiLU()\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_ENCODER = context_encoder().to(device)\n",
    "\n",
    "print(CONTEXT_ENCODER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4841b",
   "metadata": {},
   "source": [
    "### H Y P E R - X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f6dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_x(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim = embed_dim, hidden_size = hidden_size, hidden_size_2 = hidden_size_2):\n",
    "        super(hyper_x, self).__init__()\n",
    "        \n",
    "        self.hype = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.hype_out = nn.Linear(hidden_size, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, encoded_z):\n",
    "        \n",
    "        hype = self.hype(encoded_z)\n",
    "        \n",
    "        out = F.silu(self.norm(self.hype_out(hype)))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea96db",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e28c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyper_x(\n",
      "  (hype): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (11): SiLU()\n",
      "  )\n",
      "  (hype_out): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "HYPER_X = hyper_x().to(device)\n",
    "\n",
    "print(HYPER_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d2606",
   "metadata": {},
   "source": [
    "### C R O S S - A T T E N T I O N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "387bfda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, embed_dim = embed_dim, hidden_size = hidden_size):\n",
    "        super(cross_attention, self).__init__()\n",
    "        \n",
    "        # Q\n",
    "        \n",
    "        self.query = nn.Linear(state_dim, hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # K \n",
    "        \n",
    "        self.key = nn.Linear(embed_dim, hidden_size)\n",
    "        self.normk = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # V \n",
    "        \n",
    "        self.value = nn.Linear(embed_dim, hidden_size)\n",
    "        self.normv = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # attn out\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, hidden_size)\n",
    "        self.norm_out = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, query, encoded_z):\n",
    "        \n",
    "        query = query.unsqueeze(0)    # [batch, 1, dim] as encoded is [ batch, context, dim ]\n",
    "        \n",
    "        Q = F.silu(self.norm(self.query(query)))         \n",
    "        K = F.silu(self.normk(self.key(encoded_z)))            \n",
    "        V = F.silu(self.normv(self.value(encoded_z)))\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "        \n",
    "        attn_out = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        attn_out = attn_out.squeeze(1)              # [ batch, dim ]\n",
    "        \n",
    "        out = F.silu(self.norm_out(self.out(attn_out)))\n",
    "        \n",
    "        return out        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8386a62",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc80d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attention(\n",
      "  (query): Linear(in_features=17, out_features=128, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (key): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (normk): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (value): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (normv): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (out): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (norm_out): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CROSS_ATTENTION = cross_attention().to(device)\n",
    "\n",
    "print(CROSS_ATTENTION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca888375",
   "metadata": {},
   "source": [
    "### A C T O R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caee1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, hidden_size = hidden_size, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action):\n",
    "        super(actor_net, self).__init__()\n",
    "        \n",
    "        # first layer\n",
    "        \n",
    "        input_dim = state_dim + hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, head_1)\n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # parchio\n",
    "        \n",
    "        self.mod = nn.Linear(hidden_size, head_2)\n",
    "        self.mod_norm = nn.LayerNorm(head_2)\n",
    "        \n",
    "        # second layer\n",
    "        \n",
    "        self.fc2 = nn.Linear(head_1, head_2)\n",
    "        self.norm2 = nn.LayerNorm(head_2)\n",
    "        \n",
    "        # third layer\n",
    "        \n",
    "        self.fc3 = nn.Linear(head_2, head_3)\n",
    "        self.norm3 = nn.LayerNorm(head_3)\n",
    "        \n",
    "        # forth layer\n",
    "        \n",
    "        self.fc4 = nn.Linear(head_3, head_4)\n",
    "        self.norm4 = nn.LayerNorm(head_4)\n",
    "        \n",
    "        # mu and log std head\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state, attn_out):\n",
    "        \n",
    "        cat = torch.cat([state, attn_out], dim = -1)\n",
    "        \n",
    "        x = F.silu(self.norm(self.fc1(cat)))\n",
    "        \n",
    "        x = F.silu(self.norm2(self.fc2(x)))\n",
    "        \n",
    "        mod = F.silu(self.mod_norm(self.mod(attn_out)))\n",
    "                \n",
    "        x = x + mod\n",
    "        \n",
    "        x = F.silu(self.norm3(self.fc3(x)))\n",
    "        \n",
    "        x = F.silu(self.norm4(self.fc4(x)))\n",
    "        \n",
    "        # mu and log std\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        \n",
    "        action = torch.tanh(mu) * self.max_action\n",
    "        \n",
    "        return action   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430f9f4",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "230667d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor_net(\n",
      "  (fc1): Linear(in_features=145, out_features=64, bias=True)\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (mod): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (mod_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (norm4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (mu): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (log_std): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ACTOR_NETWORK = actor_net().to(device)\n",
    "\n",
    "TARGET_ACTOR = copy.deepcopy(ACTOR_NETWORK).to(device)\n",
    "\n",
    "print(ACTOR_NETWORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad8308",
   "metadata": {},
   "source": [
    "### C R I T I C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b8e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, hidden_size = hidden_size, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4):\n",
    "        super(critic_net, self).__init__()\n",
    "        \n",
    "        # first layer\n",
    "        \n",
    "        input_dim = state_dim + action_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, head_1)\n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # second layer\n",
    "        \n",
    "        self.fc2 = nn.Linear(head_1, head_2)\n",
    "        self.norm2 = nn.LayerNorm(head_2)\n",
    "        \n",
    "        # modulation\n",
    "        \n",
    "        self.mod = nn.Linear(hidden_size, head_2)\n",
    "        self.norm_mod = nn.LayerNorm(head_2)\n",
    "        \n",
    "        # third layer\n",
    "        \n",
    "        self.fc3 = nn.Linear(head_2, head_3)\n",
    "        self.norm3 = nn.LayerNorm(head_3)\n",
    "        \n",
    "        # forth layer\n",
    "        \n",
    "        self.fc4 = nn.Linear(head_3, head_4)\n",
    "        self.norm4 = nn.LayerNorm(head_4)\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        self.critic_head = nn.Linear(head_4, 1)\n",
    "        self.critic_head_2 = nn.Linear(head_4, 1)\n",
    "        \n",
    "    def forward(self, state, action, attn_out):\n",
    "        \n",
    "        # cat\n",
    "        \n",
    "        cat = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        x = F.silu(self.norm(self.fc1(cat)))\n",
    "        \n",
    "        x = F.silu(self.norm2(self.fc2(x)))\n",
    "        \n",
    "        # modulation\n",
    "        \n",
    "        mod = F.silu(self.norm_mod(self.mod(attn_out)))\n",
    "        x = x + mod\n",
    "        \n",
    "        # continue non linearity\n",
    "        \n",
    "        x = F.silu(self.norm3(self.fc3(x)))\n",
    "        \n",
    "        x = F.silu(self.norm4(self.fc4(x)))\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        q1 = self.critic_head(x)\n",
    "        q2 = self.critic_head_2(x)\n",
    "        \n",
    "        return q1, q2        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbb5d1",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f807982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_net(\n",
      "  (fc1): Linear(in_features=23, out_features=64, bias=True)\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mod): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (norm_mod): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (norm4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (critic_head): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (critic_head_2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CRITIC_NETWORK = critic_net().to(device)\n",
    "\n",
    "TARGET_CRITIC = copy.deepcopy(CRITIC_NETWORK).to(device)\n",
    "\n",
    "print(CRITIC_NETWORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886849e",
   "metadata": {},
   "source": [
    "### O P T I M I Z E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1fe6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "context_lr = 3e-5\n",
    "critic_lr = 1e-4\n",
    "actor_lr = 3e-4\n",
    "hyper_x_lr = 3e-5\n",
    "cross_attention_lr = 1e-5\n",
    "\n",
    "T_max = 2800\n",
    "\n",
    "# param\n",
    "\n",
    "actor_params = ACTOR_NETWORK.parameters()\n",
    "critic_params = CRITIC_NETWORK.parameters()\n",
    "context_params = CONTEXT_ENCODER.parameters()\n",
    "hyper_x_params = HYPER_X.parameters()\n",
    "cross_attention_params = CROSS_ATTENTION.parameters()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "CRITIC_OPTIMIZER = optim.AdamW([\n",
    "    {'params': critic_params, 'lr': critic_lr, 'weight_decay': 1e-5}\n",
    "])\n",
    "\n",
    "OPTIMIZER = optim.AdamW([\n",
    "    \n",
    "    {'params': context_params, 'lr': context_lr, 'weight_decay': 1e-5},\n",
    "    \n",
    "    {'params': hyper_x_params, 'lr': hyper_x_lr, 'weight_decay': 1e-5},\n",
    "    \n",
    "    {'params': cross_attention_params, 'lr': cross_attention_lr, 'weight_decay': 1e-5},\n",
    "    \n",
    "    {'params': actor_params, 'lr': actor_lr, 'weight_decay': 0}  \n",
    "    \n",
    "])\n",
    "\n",
    "# scheduler\n",
    "\n",
    "CRITIC_SCHEDULER = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    CRITIC_OPTIMIZER,\n",
    "    T_max=200,\n",
    "    eta_min=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max, eta_min = 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579be3ec",
   "metadata": {},
   "source": [
    "### B U F F E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04a4fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_buffer:\n",
    "    \n",
    "    def __init__(self, max_episodes):\n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.current_episode = []\n",
    "        self.episodes = []\n",
    "        \n",
    "    def add(self, state, action, reward, done, next_state):\n",
    "        \n",
    "        # safe tensor all\n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = safe_tensor(action)\n",
    "        reward = safe_tensor(reward)\n",
    "        done = safe_tensor(done)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        \n",
    "        # add \n",
    "        \n",
    "        self.current_episode.append({\n",
    "            \n",
    "            'states': state,\n",
    "            'actions': action,\n",
    "            'rewards': reward,\n",
    "            'dones': done,\n",
    "            'next_states': next_state\n",
    "        })\n",
    "        \n",
    "        if done.item()  == 1:\n",
    "            \n",
    "            self.episodes.append(self.current_episode)\n",
    "            self.current_episode = []\n",
    "            \n",
    "        if len(self.episodes) > self.max_episodes:\n",
    "            \n",
    "            for _ in range(5):\n",
    "            \n",
    "                self.episodes.pop(0)\n",
    "            \n",
    "    def sample(self, batch_size, fixed_length):\n",
    "        \n",
    "        sampled_ep = random.sample(self.episodes, k=min(batch_size, len(self.episodes)))\n",
    "        \n",
    "        segments = []\n",
    "        masks = []\n",
    "        \n",
    "        for ep in sampled_ep:\n",
    "            \n",
    "            if len(ep) >= fixed_length:\n",
    "                \n",
    "                seg = ep[:fixed_length]\n",
    "                mask = torch.ones(fixed_length, dtype = torch.float32)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                seg, mask = self.pad_episode(ep, fixed_length)\n",
    "                \n",
    "            segments.append(seg)\n",
    "            masks.append(mask)\n",
    "            \n",
    "            \n",
    "        def stack_field(x):\n",
    "        \n",
    "            return torch.stack([torch.stack([step[x] for step in seg]) for seg in segments])\n",
    "        \n",
    "        batch = {\n",
    "            \n",
    "            'states': stack_field('states'),\n",
    "            'actions': stack_field('actions'),\n",
    "            'rewards': stack_field('rewards'),\n",
    "            'dones': stack_field('dones'),\n",
    "            'next_states': stack_field('next_states'),\n",
    "            'masks': torch.stack(masks).to(device)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        for k in batch:\n",
    "            \n",
    "            batch[k] = batch[k].to(device)\n",
    "        \n",
    "        return batch\n",
    "            \n",
    "    def pad_episode(self, ep, fixed_length):\n",
    "        \n",
    "        pad_length = fixed_length - len(ep)\n",
    "        \n",
    "        last_step = ep[-1]\n",
    "        \n",
    "        pad_step = {}\n",
    "        \n",
    "        for k, v in last_step.items():\n",
    "            \n",
    "            if torch.is_tensor(v):\n",
    "                \n",
    "                pad_step[k] = v.clone()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                pad_step[k] = torch.zeros_like(v)\n",
    "                \n",
    "        \n",
    "        mask = torch.cat([\n",
    "            \n",
    "          torch.ones(len(ep), dtype = torch.float32),\n",
    "          torch.zeros(pad_length, dtype = torch.float32)  \n",
    "            \n",
    "        ])\n",
    "        \n",
    "        return ep + [pad_step] * pad_length, mask\n",
    "    \n",
    "    def clear(self):\n",
    "        \n",
    "        self.episodes.clear()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f3c1b",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0331968",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 300\n",
    "\n",
    "buffer = meta_buffer(max_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c9d6c",
   "metadata": {},
   "source": [
    "### M E T A - R U N N E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecbbf626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_runner:\n",
    "    \n",
    "    def __init__(self, max_episode_length, env = env, buffer = buffer, CONTEXT_ENCODER = CONTEXT_ENCODER, HYPER_X = HYPER_X, CROSS_ATTENTION = CROSS_ATTENTION, ACTOR_NETWORK = ACTOR_NETWORK):\n",
    "    \n",
    "        # network\n",
    "        \n",
    "        self.encoder = CONTEXT_ENCODER\n",
    "        self.hyper = HYPER_X\n",
    "        self.attention = CROSS_ATTENTION\n",
    "        self.actor =  ACTOR_NETWORK\n",
    "        \n",
    "        # buffer\n",
    "        \n",
    "        self.buffer = buffer\n",
    "        \n",
    "        # episodes\n",
    "        \n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "        # env\n",
    "                \n",
    "        self.env = env\n",
    "        \n",
    "    def run(self, num_tasks):\n",
    "        \n",
    "        tasks = self.env.sample_tasks(num_tasks)\n",
    "        \n",
    "        for task in tasks:\n",
    "        \n",
    "            self.env.set_task(task)\n",
    "            obs = self.env.reset()\n",
    "            obs = safe_tensor(obs).unsqueeze(0)\n",
    "            \n",
    "            context = []\n",
    "            \n",
    "            for _ in range(self.max_ep_length):\n",
    "                \n",
    "                latent_z = self.get_latent(context)    \n",
    "                \n",
    "                hyper = self.hyper.forward(latent_z)\n",
    "                    \n",
    "                attn_out = self.attention.forward(obs, hyper)\n",
    "                    \n",
    "                action = self.actor.forward(obs, attn_out)\n",
    "                    \n",
    "                action_np = action.detach().cpu().numpy()[0]\n",
    "                    \n",
    "                next_state, reward, done = self.env.step(action_np)\n",
    "                    \n",
    "                next_state = safe_tensor(next_state).unsqueeze(0)\n",
    "                reward = safe_tensor([reward]).unsqueeze(0)\n",
    "                    \n",
    "                self.buffer.add(obs.squeeze(0), action.squeeze(0), reward.squeeze(0), float(done), next_state.squeeze(0))\n",
    "                    \n",
    "                context.append({\n",
    "                    \n",
    "                    'states': obs,\n",
    "                    'actions': action,\n",
    "                    'rewards': reward,\n",
    "                    'next_states': next_state\n",
    "\n",
    "                })    \n",
    "                    \n",
    "                obs = next_state\n",
    "                    \n",
    "                if done:\n",
    "                        \n",
    "                    break\n",
    "                    \n",
    "    def build_context(self, context):\n",
    "        \n",
    "        return {\n",
    "            \n",
    "            'states': torch.cat([e['states'] for e in context], dim = 0),\n",
    "            'actions': torch.cat([e['actions'] for e in context], dim = 0),\n",
    "            'rewards': torch.cat([e['rewards'] for e in context], dim = 0),\n",
    "            'next_states': torch.cat([e['next_states'] for e in context], dim = 0)\n",
    "        }\n",
    "        \n",
    "    def get_latent(self, context):\n",
    "        \n",
    "        if len(context) != 0 :\n",
    "            \n",
    "            ctx = self.build_context(context)\n",
    "            latent_z = self.encoder.forward(ctx['states'], ctx['actions'], ctx['rewards'], ctx['next_states'])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            latent_z = torch.zeros((1, embed_dim)).to(device)\n",
    "        \n",
    "        return latent_z\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f7c5b",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e33a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_RUNNER = meta_runner(max_episode_length = 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941cd8b4",
   "metadata": {},
   "source": [
    "### L O S S - F U N C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c75e2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    \n",
    "    def __init__(self, tau, gamma, noise_clip, policy_noise, policy_delay, max_action = max_action, ACTOR_NETWORK = ACTOR_NETWORK, CRITIC_NETWORK = CRITIC_NETWORK, HYPER_X = HYPER_X, CROSS_ATTENTION = CROSS_ATTENTION, CONTEXT_ENCODER = CONTEXT_ENCODER, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER, buffer = buffer, CRITIC_OPTIMIZER = CRITIC_OPTIMIZER, CRITIC_SCHEDULER = CRITIC_SCHEDULER, TARGET_ACTOR = TARGET_ACTOR, TARGET_CRITIC = TARGET_CRITIC):\n",
    "        \n",
    "        # networks\n",
    "        \n",
    "        self.actor = ACTOR_NETWORK\n",
    "        self.critic = CRITIC_NETWORK\n",
    "        self.hyper = HYPER_X\n",
    "        self.attention = CROSS_ATTENTION\n",
    "        self.encoder = CONTEXT_ENCODER\n",
    "        \n",
    "        self.target_actor = TARGET_ACTOR\n",
    "        self.target_critic = TARGET_CRITIC\n",
    "        \n",
    "        # buffer\n",
    "        \n",
    "        self.buffer = buffer\n",
    "        \n",
    "        # optimizer\n",
    "        \n",
    "        self.optimizer = OPTIMIZER\n",
    "        self.schduler = SCHEDULER\n",
    "        self.critic_opt = CRITIC_OPTIMIZER\n",
    "        self.critic_sch = CRITIC_SCHEDULER\n",
    "        \n",
    "        # hyper params\n",
    "        \n",
    "        self.step = 0\n",
    "        self.actor_update = 0\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_noise = policy_noise\n",
    "        self.max_action = max_action\n",
    "        self.policy_delay = policy_delay\n",
    "        \n",
    "    def soft_update(self, source, target):\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            \n",
    "            for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "                \n",
    "                target_param.data.copy_(param * self.tau + target_param * (1 - self.tau))\n",
    "                \n",
    "    def critic_loss(self, states, actions, rewards, dones, next_states, hyper, attn_out):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            next_attn = self.attention.forward(next_states, hyper)\n",
    "            next_attn = next_attn.squeeze(0)\n",
    "            next_action = self.target_actor.forward(next_states, next_attn) \n",
    "            noise = (torch.randn_like(next_action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "            target_1, target_2 = self.target_critic.forward(next_states, next_action, next_attn)\n",
    "            target_val = torch.min(target_1, target_2)\n",
    "            target_q = rewards + self.gamma * (1 - dones) * target_val\n",
    "        \n",
    "        c1, c2 = self.critic.forward(states, actions.detach(), attn_out.detach())\n",
    "        \n",
    "        loss = F.mse_loss(c1, target_q) + F.mse_loss(c2, target_q)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def actor_loss(self, states, attn_out):\n",
    "        \n",
    "        new_action = self.actor.forward(states, attn_out.detach())\n",
    "        q1, _ = self.critic.forward(states, new_action.detach(), attn_out.detach())\n",
    "        \n",
    "        actor_loss = -q1.mean()\n",
    "        \n",
    "        return actor_loss\n",
    "    \n",
    "    def update(self, batch_size, fixed_length):\n",
    "        \n",
    "        self.step += 1\n",
    "\n",
    "        batch = self.buffer.sample(batch_size, fixed_length)\n",
    "\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        dones = batch['dones'].unsqueeze(2)\n",
    "        next_states = batch['next_states']\n",
    "        \n",
    "        # Encoder\n",
    "        \n",
    "        latent_z_critic = self.encoder.forward(states, actions, rewards, next_states)\n",
    "        \n",
    "        # hyper\n",
    "        \n",
    "        hyper_critic = self.hyper.forward(latent_z_critic)\n",
    "        \n",
    "        # attention\n",
    "        \n",
    "        attn_critic = self.attention.forward(states, latent_z_critic.detach())\n",
    "        \n",
    "        attn_critic = attn_critic.squeeze(0)\n",
    "        \n",
    "        # critic loss\n",
    "        \n",
    "        critic_loss = self.critic_loss(states, actions, rewards, dones, next_states, hyper_critic, attn_critic)\n",
    "        \n",
    "        # optimize critic\n",
    "        \n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm = 0.5)\n",
    "        self.critic_opt.step()\n",
    "        self.schduler.step()\n",
    "        \n",
    "        # actor loss\n",
    "        \n",
    "        actor_loss = torch.tensor(0.0)\n",
    "        \n",
    "        if self.step % self.policy_delay == 0:\n",
    "            \n",
    "            latent_Z_actor = self.encoder.forward(states, actions, rewards, next_states)\n",
    "            \n",
    "            hyper_Actor = self.hyper.forward(latent_Z_actor)\n",
    "            \n",
    "            attn_out = self.attention.forward(states, hyper_Actor.detach())\n",
    "            \n",
    "            attn_out = attn_out.squeeze(0)\n",
    "            \n",
    "            actor_loss = self.actor_loss(states, attn_out.detach())\n",
    "            \n",
    "            # optimize\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm = 0.5)\n",
    "            torch.nn.utils.clip_grad_norm_(self.hyper.parameters(), max_norm = 0.5)\n",
    "            torch.nn.utils.clip_grad_norm_(self.attention.parameters(), max_norm = 0.5)\n",
    "            torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), max_norm = 0.5)\n",
    "            self.optimizer.step()\n",
    "            self.schduler.step()\n",
    "            \n",
    "            # soft update\n",
    "            \n",
    "            self.soft_update(self.actor, self.target_actor)\n",
    "            \n",
    "        self.soft_update(self.critic, self.target_critic)\n",
    "        \n",
    "        return actor_loss, critic_loss, self.step\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317952a2",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a747f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "noise_clip = 0.3\n",
    "policy_noise = 0.2\n",
    "policy_delay = 2\n",
    "\n",
    "\n",
    "# setup\n",
    "\n",
    "LOSS_FUNCTION = loss_func(tau, gamma, noise_clip, policy_noise, policy_delay = policy_delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4e7f8",
   "metadata": {},
   "source": [
    "### T R A I N I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5056c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs = 20, mini_batch = 64, num_tasks = 10, batch_size = 512, fixed_length = 512, buffer = buffer):\n",
    "\n",
    "\n",
    "    total_actor_loss, total_critic_loss = 0.0, 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        total_actor_loss, total_critic_loss = 0.0, 0.0\n",
    "        \n",
    "        LOSS_FUNCTION.step = 0\n",
    "        \n",
    "        for _ in range(mini_batch):\n",
    "            \n",
    "            buffer.clear()\n",
    "            \n",
    "            META_RUNNER.run(num_tasks)\n",
    "            \n",
    "            actor_loss, critic_loss, actor_update = LOSS_FUNCTION.update(batch_size, fixed_length)\n",
    "            \n",
    "            total_actor_loss += actor_loss.item()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "            \n",
    "        avg_actor_loss = total_actor_loss / actor_update\n",
    "        avg_critic_loss = total_critic_loss / mini_batch\n",
    "        \n",
    "        writer.add_scalar('Actor loss', avg_actor_loss, epoch)\n",
    "        writer.add_scalar('Critic loss', avg_critic_loss, epoch)\n",
    "        \n",
    "        writer.flush()\n",
    "        \n",
    "        print(f'epoch: {epoch} | avg actor loss: {avg_actor_loss:.3f} | avg critic loss: {avg_critic_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68fce132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | avg actor loss: 0.457 | avg critic loss: 6.416\n",
      "epoch: 1 | avg actor loss: 0.744 | avg critic loss: 2.297\n",
      "epoch: 2 | avg actor loss: 0.860 | avg critic loss: 1.715\n",
      "epoch: 3 | avg actor loss: 0.967 | avg critic loss: 1.157\n",
      "epoch: 4 | avg actor loss: 0.909 | avg critic loss: 0.759\n",
      "epoch: 5 | avg actor loss: 0.578 | avg critic loss: 0.527\n",
      "epoch: 6 | avg actor loss: 0.426 | avg critic loss: 0.414\n",
      "epoch: 7 | avg actor loss: 0.351 | avg critic loss: 0.311\n",
      "epoch: 8 | avg actor loss: 0.367 | avg critic loss: 0.288\n",
      "epoch: 9 | avg actor loss: 0.333 | avg critic loss: 0.234\n",
      "epoch: 10 | avg actor loss: 0.373 | avg critic loss: 0.203\n",
      "epoch: 11 | avg actor loss: 0.302 | avg critic loss: 0.152\n",
      "epoch: 12 | avg actor loss: 0.242 | avg critic loss: 0.148\n",
      "epoch: 13 | avg actor loss: 0.189 | avg critic loss: 0.113\n",
      "epoch: 14 | avg actor loss: 0.116 | avg critic loss: 0.141\n",
      "epoch: 15 | avg actor loss: 0.049 | avg critic loss: 0.089\n",
      "epoch: 16 | avg actor loss: 0.005 | avg critic loss: 0.080\n",
      "epoch: 17 | avg actor loss: -0.029 | avg critic loss: 0.089\n",
      "epoch: 18 | avg actor loss: -0.093 | avg critic loss: 0.070\n",
      "epoch: 19 | avg actor loss: -0.117 | avg critic loss: 0.076\n"
     ]
    }
   ],
   "source": [
    "train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
