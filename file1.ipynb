{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c77e99f",
   "metadata": {},
   "source": [
    "# F I R S T - O R D E R - M A M L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0baab",
   "metadata": {},
   "source": [
    "### D E V I C E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572ec2b",
   "metadata": {},
   "source": [
    "### H E L P E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55f599",
   "metadata": {},
   "source": [
    "### L O G G I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/FOMAML')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91301d75",
   "metadata": {},
   "source": [
    "### M E T A - E N V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb385f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_car_continuous:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.base_env = gym.make('MountainCarContinuous-v0')\n",
    "        self.org_gravity = 0.0025\n",
    "        self.org_goal_position = 0.45\n",
    "        self.tasks = []\n",
    "        self.current_task = (self.org_goal_position, self.org_gravity)\n",
    "        \n",
    "    def sample_task(self, num_tasks):\n",
    "        \n",
    "        self.tasks = []\n",
    "        \n",
    "        for _ in range(num_tasks):\n",
    "            goal_position = np.random.uniform(0.45, 0.55)\n",
    "            gravity = np.random.uniform(0.0025, 0.006)\n",
    "            self.tasks.append((goal_position, gravity))\n",
    "            \n",
    "        return self.tasks\n",
    "\n",
    "    def set_task(self, task):\n",
    "        \n",
    "        self.current_task = task\n",
    "        \n",
    "        self.base_env.env.gravity = task[1]\n",
    "        self.base_env.env.goal_position = task[0]\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        obs = self.base_env.reset()\n",
    "        \n",
    "        if isinstance(obs, tuple):\n",
    "            \n",
    "            obs = obs[0]\n",
    "            \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        step_output = self.base_env.step(action)\n",
    "        \n",
    "        if len(step_output) == 5: \n",
    "            \n",
    "            next_obs, reward, terminated, truncated, info = step_output\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            next_obs, reward, done, info = step_output\n",
    "            \n",
    "        return next_obs, reward, done, info\n",
    "\n",
    "    def get_number(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = float(self.base_env.action_space.high[0])\n",
    "        reward_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, reward_dim\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767a6fe",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec91fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: (2,)\n",
      "obs: (2,)\n",
      "state dim: 2 | action dim: 1 | max action: 1.0 | reward dim: 1\n"
     ]
    }
   ],
   "source": [
    "# Initiate env\n",
    "\n",
    "env = meta_car_continuous()\n",
    "\n",
    "tasks = env.sample_task(2)\n",
    "\n",
    "for task in tasks:\n",
    "    \n",
    "    env.set_task(task)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    print(f'obs: {obs.shape}')\n",
    "\n",
    "# get number\n",
    "\n",
    "state_dim, action_dim, max_action, reward_dim = env.get_number()\n",
    "\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action} | reward dim: {reward_dim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e144bf",
   "metadata": {},
   "source": [
    "### A S S E M B L Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad887f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 32\n",
    "head_2 = 64\n",
    "head_3 = 64\n",
    "head_4 = 32\n",
    "\n",
    "hidden_size = 32\n",
    "hidden_size_2 = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0297e",
   "metadata": {},
   "source": [
    "### H Y P E R - X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d022929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_x(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, hidden_size = hidden_size, hidden_size_2 = hidden_size_2):\n",
    "        super(hyper_x, self).__init__()\n",
    "        \n",
    "        # input dim\n",
    "        \n",
    "        input_dim = state_dim\n",
    "        \n",
    "        # mlp\n",
    "        \n",
    "        self.hyper = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        hyper = self.hyper(state)\n",
    "        \n",
    "        return hyper\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6409c7b4",
   "metadata": {},
   "source": [
    "### P O L I C Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, action_dim = action_dim, max_action = max_action):\n",
    "        super(policy_net, self).__init__()\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # hyper\n",
    "        \n",
    "        self.hyper = hyper_x()\n",
    "        \n",
    "        # mlp : [ input -> state -> hyper -> output -> hidden  [ 64 ] -> input -> policy -> head 1 [ 64 ] -> output -> action ]\n",
    "        \n",
    "        self.policy = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # mu and log std head\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # state to hyper\n",
    "        \n",
    "        hyper = self.hyper.forward(state)\n",
    "        \n",
    "        # input from hyper to policy\n",
    "        \n",
    "        x = self.policy(hyper)\n",
    "        \n",
    "        # mu  and log std\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = log_std.clamp(-10, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        z = dist.rsample()\n",
    "        tanh_z = torch.tanh(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        log_prob = dist.log_prob(z)\n",
    "        log_prob = log_prob - (1 - tanh_z.pow(2) + 1e-6).log()\n",
    "        log_prob = log_prob.sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e03f8",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b272b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_net(\n",
      "  (hyper): hyper_x(\n",
      "    (hyper): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "      (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (10): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (policy): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (log_std): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "POLICY_NETWORK = policy_net().to(device)\n",
    "\n",
    "print(POLICY_NETWORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f1360",
   "metadata": {},
   "source": [
    "### O P T I M I Z E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "\n",
    "meta_lr = 1e-4\n",
    "meta_iteration = 50\n",
    "\n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW(POLICY_NETWORK.parameters(), meta_lr, weight_decay = 0)\n",
    "SCHEDULER = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = meta_iteration, eta_min = 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34b8d1",
   "metadata": {},
   "source": [
    "### B U F F E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654aa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class roller_buffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, state, action, log_prob, reward, next_state):\n",
    "    \n",
    "        # add to buffer\n",
    "        \n",
    "        self.buffer.append({\n",
    "            \n",
    "            'states': safe_tensor(state),\n",
    "            'actions': safe_tensor(action),\n",
    "            'log_probs': safe_tensor(log_prob),\n",
    "            'rewards': safe_tensor(reward),\n",
    "            'next_states': safe_tensor(next_state)\n",
    "        })\n",
    "        \n",
    "    def safe_stack(self, x):\n",
    "        \n",
    "        return torch.stack(x).to(device)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        # take trajectories\n",
    "        \n",
    "        states = [i['states'] for i in self.buffer]\n",
    "        actions = [i['actions'] for i in self.buffer]\n",
    "        log_probs = [i['log_probs'] for i in self.buffer]\n",
    "        rewards = [i['rewards'] for i in self.buffer]\n",
    "        next_states = [i['next_states'] for i in self.buffer]\n",
    "        \n",
    "        # form batch\n",
    "        \n",
    "        batch = {\n",
    "            \n",
    "            'states': self.safe_stack(states),\n",
    "            'actions': self.safe_stack(actions),\n",
    "            'log_probs': self.safe_stack(log_probs),\n",
    "            'rewards': self.safe_stack(rewards),\n",
    "            'next_states': self.safe_stack(next_states)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def clear(self):\n",
    "        \n",
    "        self.buffer.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f05a15",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa29d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = roller_buffer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302545b5",
   "metadata": {},
   "source": [
    "### C O L L E C T - T R A J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49bd795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect_traj:\n",
    "    \n",
    "    def __init__(self, buffer = buffer, env = env, POLICY_NETWORK = POLICY_NETWORK):\n",
    "        \n",
    "        self.buffer = buffer\n",
    "        self.env = env\n",
    "        self.policy = POLICY_NETWORK\n",
    "        \n",
    "    def run(self, steps):\n",
    "            \n",
    "        obs = self.env.reset()\n",
    "            \n",
    "        obs = safe_tensor(obs).unsqueeze(0)\n",
    "            \n",
    "        total_reward = 0.0\n",
    "            \n",
    "        for _ in range(steps):\n",
    "                \n",
    "            action, log_prob = self.policy.forward(obs)\n",
    "                \n",
    "            action_np = action.detach().cpu().numpy()[0]\n",
    "                \n",
    "            next_obs, reward, done, _ = self.env.step(action_np)\n",
    "                \n",
    "            total_reward += reward\n",
    "                \n",
    "            next_obs = safe_tensor(next_obs).unsqueeze(0)\n",
    "                \n",
    "            self.buffer.add(obs.squeeze(0), action.squeeze(0), log_prob.squeeze(0), reward, next_obs.squeeze(0))\n",
    "            \n",
    "            obs = next_obs\n",
    "                \n",
    "            if done:\n",
    "                    \n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34a0e9",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ef490",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECT_TRAJECTORY = collect_traj()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbdfe3",
   "metadata": {},
   "source": [
    "### A D A P T - P O L I C Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_policy(learning_rate, POLICY_NETWORK = POLICY_NETWORK, buffer = buffer):\n",
    "    \n",
    "    # clone the policy\n",
    "    \n",
    "    adapted_policy = copy.deepcopy(POLICY_NETWORK)\n",
    "    \n",
    "    # Get batch from buffer\n",
    "    \n",
    "    batch = buffer.sample()\n",
    "    \n",
    "    states = batch['states']\n",
    "    actions = batch['actions']\n",
    "    rewards = batch['rewards']\n",
    "    \n",
    "    rewards = rewards.unsqueeze(1)\n",
    "    \n",
    "    # compute log probs from cloned policy\n",
    "    \n",
    "    _, log_probs = adapted_policy.forward(states)\n",
    "    \n",
    "    # compute reinforce loss\n",
    "    \n",
    "    loss = - (log_probs.squeeze() * rewards.squeeze()).mean()\n",
    "    \n",
    "    # compute gradients\n",
    "    \n",
    "    grad = torch.autograd.grad(loss, adapted_policy.parameters(), create_graph = True)\n",
    "    \n",
    "    # gradient step\n",
    "    \n",
    "    for p, g in zip(adapted_policy.parameters(), grad):\n",
    "        \n",
    "        p.data = p.data - learning_rate * g\n",
    "        \n",
    "    return adapted_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c36006",
   "metadata": {},
   "source": [
    "### C O L L E C T - V A L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_val(adapted_policy, task, steps, env = env):\n",
    "      \n",
    "    env.set_task(task)\n",
    "        \n",
    "    obs = env.reset()\n",
    "    obs = safe_tensor(obs).unsqueeze(0)\n",
    "        \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for _ in range(steps):\n",
    "            \n",
    "        action, log_prob = adapted_policy.forward(obs)\n",
    "            \n",
    "        action_np = action.detach().cpu().numpy()[0]\n",
    "            \n",
    "        next_obs, reward, done, _ = env.step(action_np)\n",
    "            \n",
    "        next_obs = safe_tensor(next_obs).unsqueeze(0)\n",
    "            \n",
    "        reward = safe_tensor(reward)\n",
    "            \n",
    "        log_prob = log_prob.squeeze()\n",
    "            \n",
    "        total_loss += ( - log_prob * reward)\n",
    "            \n",
    "        obs = next_obs\n",
    "            \n",
    "        if done:\n",
    "                \n",
    "            break\n",
    "            \n",
    "    val_loss = total_loss / steps\n",
    "    return val_loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577c5c2",
   "metadata": {},
   "source": [
    "### T R A I N I N G - L O O P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed93f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_runner_loop(num_tasks, steps, learning_rate, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER, meta_iteration = meta_iteration):\n",
    "    \n",
    "    for iteration in tqdm(range(1, meta_iteration + 1), desc = 'FOMAML'):\n",
    "        \n",
    "        OPTIMIZER.zero_grad()\n",
    "        total_val_loss = 0.0\n",
    "        \n",
    "        tasks = env.sample_task(num_tasks)\n",
    "        \n",
    "        for task_num in tasks:\n",
    "            \n",
    "            env.set_task(task_num)\n",
    "            \n",
    "            buffer.clear()\n",
    "            \n",
    "            COLLECT_TRAJECTORY.run(steps = steps)\n",
    "            \n",
    "            adapted_policy = adapt_policy(learning_rate)\n",
    "            \n",
    "            val_loss = collect_val(adapted_policy, task_num, steps)\n",
    "            \n",
    "            val_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(POLICY_NETWORK.parameters(), max_norm = 0.5)\n",
    "            \n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "        OPTIMIZER.step()\n",
    "        SCHEDULER.step()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / num_tasks\n",
    "        \n",
    "        writer.add_scalar('VALIDATION Loss', avg_val_loss, iteration)\n",
    "        \n",
    "        print(f'epoch: {iteration} | avg_val_loss: {avg_val_loss:.4f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e31b0",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6497635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:   2%|▏         | 1/50 [00:03<03:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | avg_val_loss: -0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:   4%|▍         | 2/50 [00:06<02:32,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | avg_val_loss: -0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:   6%|▌         | 3/50 [00:10<02:39,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | avg_val_loss: -0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:   8%|▊         | 4/50 [00:12<02:21,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | avg_val_loss: -0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  10%|█         | 5/50 [00:15<02:06,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | avg_val_loss: -0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  12%|█▏        | 6/50 [00:19<02:26,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | avg_val_loss: -0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  14%|█▍        | 7/50 [00:22<02:19,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | avg_val_loss: -0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  16%|█▌        | 8/50 [00:24<02:03,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | avg_val_loss: -0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  18%|█▊        | 9/50 [00:27<01:56,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | avg_val_loss: -0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  20%|██        | 10/50 [00:30<01:52,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 | avg_val_loss: -0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  22%|██▏       | 11/50 [00:33<01:57,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 | avg_val_loss: -0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  24%|██▍       | 12/50 [00:36<01:48,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 | avg_val_loss: -0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  26%|██▌       | 13/50 [00:38<01:38,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 | avg_val_loss: -0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  28%|██▊       | 14/50 [00:40<01:32,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 | avg_val_loss: -0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  30%|███       | 15/50 [00:42<01:27,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 | avg_val_loss: -0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  32%|███▏      | 16/50 [00:45<01:23,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | avg_val_loss: -0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  34%|███▍      | 17/50 [00:47<01:20,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 | avg_val_loss: -0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  36%|███▌      | 18/50 [00:49<01:15,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 | avg_val_loss: -0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  38%|███▊      | 19/50 [00:52<01:11,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 | avg_val_loss: -0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  40%|████      | 20/50 [00:54<01:10,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 | avg_val_loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  42%|████▏     | 21/50 [00:57<01:09,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 | avg_val_loss: -0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  44%|████▍     | 22/50 [00:59<01:05,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 | avg_val_loss: -0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  46%|████▌     | 23/50 [01:01<01:04,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 | avg_val_loss: -0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  48%|████▊     | 24/50 [01:05<01:12,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 | avg_val_loss: -0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  50%|█████     | 25/50 [01:07<01:05,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 | avg_val_loss: -0.0057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  52%|█████▏    | 26/50 [01:10<01:01,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 | avg_val_loss: -0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  54%|█████▍    | 27/50 [01:12<00:56,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 | avg_val_loss: -0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  56%|█████▌    | 28/50 [01:14<00:51,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 | avg_val_loss: -0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  58%|█████▊    | 29/50 [01:16<00:48,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 | avg_val_loss: -0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  60%|██████    | 30/50 [01:19<00:46,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 | avg_val_loss: -0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  62%|██████▏   | 31/50 [01:21<00:44,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31 | avg_val_loss: -0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  64%|██████▍   | 32/50 [01:23<00:41,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 | avg_val_loss: -0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  66%|██████▌   | 33/50 [01:25<00:38,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33 | avg_val_loss: -0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  68%|██████▊   | 34/50 [01:27<00:35,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34 | avg_val_loss: -0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  70%|███████   | 35/50 [01:30<00:33,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35 | avg_val_loss: -0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  72%|███████▏  | 36/50 [01:32<00:31,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36 | avg_val_loss: -0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  74%|███████▍  | 37/50 [01:34<00:29,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37 | avg_val_loss: -0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  76%|███████▌  | 38/50 [01:37<00:27,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38 | avg_val_loss: -0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  78%|███████▊  | 39/50 [01:39<00:24,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39 | avg_val_loss: -0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  80%|████████  | 40/50 [01:41<00:22,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40 | avg_val_loss: -0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  82%|████████▏ | 41/50 [01:43<00:20,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41 | avg_val_loss: -0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  84%|████████▍ | 42/50 [01:46<00:18,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42 | avg_val_loss: -0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  86%|████████▌ | 43/50 [01:48<00:15,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43 | avg_val_loss: -0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  88%|████████▊ | 44/50 [01:50<00:13,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44 | avg_val_loss: -0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  90%|█████████ | 45/50 [01:52<00:10,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 | avg_val_loss: -0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  92%|█████████▏| 46/50 [01:54<00:08,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 | avg_val_loss: -0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  94%|█████████▍| 47/50 [01:56<00:06,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47 | avg_val_loss: -0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  96%|█████████▌| 48/50 [01:58<00:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48 | avg_val_loss: -0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML:  98%|█████████▊| 49/50 [02:01<00:02,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 | avg_val_loss: -0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FOMAML: 100%|██████████| 50/50 [02:03<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 | avg_val_loss: -0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_tasks = 5\n",
    "steps = 64\n",
    "learning_rate = 1e-5\n",
    "\n",
    "meta_runner_loop(num_tasks, steps, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
