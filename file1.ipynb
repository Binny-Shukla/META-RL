{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f2b6b7",
   "metadata": {},
   "source": [
    "# **Almost No Inner Loop** Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10506455",
   "metadata": {},
   "source": [
    "### **LOGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/ANIL')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3db1c7",
   "metadata": {},
   "source": [
    "### **DEVICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b869de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c460f0",
   "metadata": {},
   "source": [
    "### **META - ENV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ae8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_car_continuous:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.base_env = gym.make('MountainCarContinuous-v0')\n",
    "        self.org_gravity = 0.0025\n",
    "        self.org_goal_position = 0.45\n",
    "        self.tasks = []\n",
    "        self.current_task = (self.org_goal_position, self.org_gravity)\n",
    "        \n",
    "    def sample_task(self, num_tasks):\n",
    "        \n",
    "        self.tasks = []\n",
    "        \n",
    "        for _ in range(num_tasks):\n",
    "            goal_position = np.random.uniform(0.45, 0.55)\n",
    "            gravity = np.random.uniform(0.0025, 0.006)\n",
    "            self.tasks.append((goal_position, gravity))\n",
    "            \n",
    "        return self.tasks\n",
    "\n",
    "    def set_task(self, task):\n",
    "        \n",
    "        self.current_task = task\n",
    "        \n",
    "        self.base_env.env.gravity = task[1]\n",
    "        self.base_env.env.goal_position = task[0]\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        obs = self.base_env.reset()\n",
    "        \n",
    "        if isinstance(obs, tuple):\n",
    "            \n",
    "            obs = obs[0]\n",
    "            \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        step_output = self.base_env.step(action)\n",
    "        \n",
    "        if len(step_output) == 5: \n",
    "            \n",
    "            next_obs, reward, terminated, truncated, info = step_output\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            next_obs, reward, done, info = step_output\n",
    "            \n",
    "        return next_obs, reward, done, info\n",
    "\n",
    "    def get_number(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = float(self.base_env.action_space.high[0])\n",
    "        reward_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, reward_dim\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe78085",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f4a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks: [(0.49934588688337816, 0.004526556866872781), (0.512269297349774, 0.0044080949375128325), (0.5375930700427028, 0.005983689153189032), (0.5159142616008038, 0.005719457998977788), (0.5225756374044043, 0.005078740307298664)]\n",
      "\n",
      "obs: (2,)\n",
      "\n",
      "state dim: 2 | action dim: 1 | max action: 1.0\n"
     ]
    }
   ],
   "source": [
    "META_ENV = meta_car_continuous()\n",
    "\n",
    "tasks = META_ENV.sample_task(num_tasks = 5)\n",
    "\n",
    "obs = META_ENV.reset()\n",
    "\n",
    "state_dim, action_dim, max_action, reward_dim = META_ENV.get_number()\n",
    "\n",
    "print(f'Tasks: {tasks}')\n",
    "print()\n",
    "print(f'obs: {obs.shape}')\n",
    "print()\n",
    "print(f'state dim: {state_dim} | action dim: {action_dim} | max action: {max_action}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91313ab6",
   "metadata": {},
   "source": [
    "### **HELPER FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f755fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a7c39",
   "metadata": {},
   "source": [
    "### **ASSEMBLY**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 32\n",
    "head_2 = 64\n",
    "head_3 = 64\n",
    "head_4 = 32\n",
    "\n",
    "hidden_size = 32\n",
    "hidden_size_2 = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11252f",
   "metadata": {},
   "source": [
    "### **HYPER X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6676ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_x(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size = hidden_size, hidden_size_2 = hidden_size_2, state_dim = state_dim):\n",
    "        super(hyper_x, self).__init__()\n",
    "        \n",
    "        # input dim\n",
    "        \n",
    "        input_dim = state_dim\n",
    "        \n",
    "        # hyper mlp\n",
    "        \n",
    "        self.hyper = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size_2),\n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        return self.hyper(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108ece8",
   "metadata": {},
   "source": [
    "### **POLICY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, action_dim = action_dim, hidden_size = hidden_size, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, max_action = max_action):\n",
    "        super(policy_net, self).__init__()\n",
    "        \n",
    "        # hyper x\n",
    "        \n",
    "        self.hyper_x = hyper_x()\n",
    "        \n",
    "        \n",
    "        assert hidden_size == head_1\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        self.policy_mlp = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.LayerNorm(head_4),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # mu and log std head\n",
    "        \n",
    "        self.mu = nn.Linear(head_4, action_dim)\n",
    "        self.log_std = nn.Linear(head_4, action_dim)\n",
    "        \n",
    "        # apply normalization\n",
    "        \n",
    "        self.apply(self.init_weight)\n",
    "        \n",
    "        # max action\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def init_weight(self, m):\n",
    "        \n",
    "        if isinstance(m, nn.Linear):\n",
    "            \n",
    "            nn.init.orthogonal_(m.weight)\n",
    "            \n",
    "            if m.bias is not None:\n",
    "                \n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # state -> hyper\n",
    "        \n",
    "        hyper = self.hyper_x.forward(state)\n",
    "        \n",
    "        # hyper -> policy mlp\n",
    "        \n",
    "        policy_mlp = self.policy_mlp(hyper)\n",
    "        \n",
    "        # mu and log std\n",
    "        \n",
    "        mu = self.mu(policy_mlp)\n",
    "        log_std = self.log_std(policy_mlp)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # reparameterization trick\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        z = dist.rsample()\n",
    "        tanh_z = torch.tanh(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        # log squashing\n",
    "        \n",
    "        log_prob = dist.log_prob(z)\n",
    "        squash = torch.log(1 - tanh_z.pow(2) + 1e-6)\n",
    "        log_prob = log_prob - squash\n",
    "        \n",
    "        log_prob = log_prob.sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab1c2b",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25883fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_net(\n",
      "  (hyper_x): hyper_x(\n",
      "    (hyper): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "      (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): SiLU()\n",
      "      (3): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): SiLU()\n",
      "      (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (8): SiLU()\n",
      "      (9): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (10): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (11): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (policy_mlp): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (log_std): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "POLICY_NET = policy_net().to(device)\n",
    "\n",
    "print(POLICY_NET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ca899",
   "metadata": {},
   "source": [
    "### **BUFFER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67531cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class buffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, state, action, log_prob, reward):\n",
    "        \n",
    "        self.buffer.append({\n",
    "            \n",
    "            'states': safe_tensor(state),\n",
    "            'actions': safe_tensor(action),\n",
    "            'log_probs': safe_tensor(log_prob),\n",
    "            'rewards': safe_tensor(reward)\n",
    "            \n",
    "        })\n",
    "        \n",
    "    def safe_stack(self, x):\n",
    "        \n",
    "        return torch.stack(x).to(device)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        # unpack\n",
    "        \n",
    "        states = [i['states'] for i in self.buffer]\n",
    "        actions = [i['actions'] for i in self.buffer]\n",
    "        log_probs = [i['log_probs'] for i in self.buffer]\n",
    "        rewards = [i['rewards'] for i in self.buffer]\n",
    "        \n",
    "        # construct batch\n",
    "        \n",
    "        batch = {\n",
    "            \n",
    "            'states': self.safe_stack(states),\n",
    "            'actions': self.safe_stack(actions),\n",
    "            'log_probs': self.safe_stack(log_probs),\n",
    "            'rewards': self.safe_stack(rewards)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def clear(self):\n",
    "        \n",
    "        self.buffer.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5a248",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_BUFFER = buffer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab74330",
   "metadata": {},
   "source": [
    "### **OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "inner_lr = 3e-4\n",
    "meta_lr = 1e-4\n",
    "\n",
    "T_max = 50\n",
    "warmup = 20\n",
    "\n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW(POLICY_NET.parameters(), meta_lr, weight_decay = 0)\n",
    "\n",
    "# scheduler\n",
    "\n",
    "warmup_sch = optim.lr_scheduler.LinearLR(OPTIMIZER, 0.1, total_iters = warmup)\n",
    "cosine_sch = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = T_max - warmup, eta_min = 1e-5)\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.SequentialLR(OPTIMIZER, [warmup_sch, cosine_sch], milestones = [warmup])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a00434",
   "metadata": {},
   "source": [
    "### **COLLECT TRAJ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc7db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect_traj:\n",
    "    \n",
    "    def __init__(self, META_ENV = META_ENV, META_BUFFER = META_BUFFER, POLICY_NET = POLICY_NET):\n",
    "        \n",
    "        self.network = POLICY_NET\n",
    "        self.env = META_ENV\n",
    "        self.buffer = META_BUFFER\n",
    "        \n",
    "    def run(self, steps):\n",
    "            \n",
    "        obs = self.env.reset()\n",
    "        obs = safe_tensor(obs).unsqueeze(0)\n",
    "            \n",
    "        for step in range(steps):\n",
    "            \n",
    "            action, log_prob = self.network.forward(obs)\n",
    "            \n",
    "            action_np = action.detach().cpu().numpy()[0]\n",
    "            \n",
    "            next_obs, reward, done, _ = self.env.step(action_np)\n",
    "            \n",
    "            next_obs = safe_tensor(next_obs).unsqueeze(0)\n",
    "            \n",
    "            self.buffer.add(obs.squeeze(0), action.squeeze(0), log_prob.squeeze(0), [reward])\n",
    "            \n",
    "            obs = next_obs\n",
    "            \n",
    "            if done:\n",
    "                \n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853a211",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85121cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECT_TRAJ = collect_traj()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb695d",
   "metadata": {},
   "source": [
    "### **ADAPTED POLICY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapted_policy(inner_lr, network = POLICY_NET, buffer = META_BUFFER):\n",
    "    \n",
    "    # adapt policy\n",
    "    \n",
    "    adapt_policy = copy.deepcopy(network).to(device)\n",
    "    \n",
    "    # now do  ANIL\n",
    "    \n",
    "    for name, param in adapt_policy.named_parameters():\n",
    "        \n",
    "        if 'mu' not in name and 'log_std' not in name:\n",
    "            \n",
    "            param.requires_grad = False\n",
    "            \n",
    "    # sample buffer\n",
    "            \n",
    "    batch = buffer.sample()\n",
    "    \n",
    "    # unpack batch\n",
    "    \n",
    "    log_probs = batch['log_probs']\n",
    "    rewards = batch['rewards']\n",
    "    \n",
    "    rewards = rewards.view(-1, 1)    \n",
    "    # loss\n",
    "    \n",
    "    loss = (- log_probs * rewards).mean()\n",
    "    \n",
    "    # compute grad\n",
    "    \n",
    "    grad = torch.autograd.grad(loss, filter(lambda p: p.requires_grad, adapt_policy.parameters()), create_graph = True, allow_unused = True)\n",
    "    \n",
    "    # inner update\n",
    "    \n",
    "    for p, g in zip(filter(lambda p: p.requires_grad , adapt_policy.parameters()), grad):\n",
    "        \n",
    "        if g is not None:\n",
    "            \n",
    "            p.data -= inner_lr * g\n",
    "        \n",
    "    return adapt_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b242b96",
   "metadata": {},
   "source": [
    "### **COLLECT - VAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e053c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_val(adapted_policy, task, steps, META_ENV = META_ENV):\n",
    "    \n",
    "    META_ENV.set_task(task)\n",
    "    \n",
    "    obs = META_ENV.reset()\n",
    "    \n",
    "    obs = safe_tensor(obs).unsqueeze(0)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        \n",
    "        action, log_prob = adapted_policy(obs)\n",
    "        \n",
    "        action_np = action.detach().cpu().numpy()[0]\n",
    "        \n",
    "        next_obs, reward, done, _ = META_ENV.step(action_np)\n",
    "        \n",
    "        reward = safe_tensor([reward])\n",
    "        \n",
    "        loss = - (log_prob.squeeze() * reward).mean()\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "        next_obs = safe_tensor(next_obs).unsqueeze(0)\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            break\n",
    "        \n",
    "    val_loss = total_loss / steps\n",
    "    \n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25d44c",
   "metadata": {},
   "source": [
    "### **TRAINING LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05c0b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(meta_iteration, steps, num_tasks, inner_lr = inner_lr, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER, META_BUFFER = META_BUFFER, META_ENV = META_ENV):\n",
    "    \n",
    "    for iteration in range(meta_iteration):\n",
    "        \n",
    "        total_val_loss = 0.0\n",
    "        \n",
    "        META_BUFFER.clear()\n",
    "        \n",
    "        tasks = META_ENV.sample_task(num_tasks)\n",
    "        \n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        for task in tasks:\n",
    "            \n",
    "            META_ENV.set_task(task)\n",
    "            \n",
    "            COLLECT_TRAJ.run(steps)\n",
    "            \n",
    "            adapt_policy = adapted_policy(inner_lr)\n",
    "            \n",
    "            val_loss = collect_val(adapt_policy, task, steps)\n",
    "            \n",
    "            val_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(POLICY_NET.parameters(), max_norm = 0.5)\n",
    "            \n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "            \n",
    "        OPTIMIZER.step()\n",
    "        SCHEDULER.step()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / num_tasks\n",
    "        \n",
    "        writer.add_scalar('Validation loss', avg_val_loss, iteration)\n",
    "        \n",
    "        writer.flush()\n",
    "        \n",
    "        print(f'epoch: {iteration} | avg_val_loss: {avg_val_loss:.4f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06d7ff",
   "metadata": {},
   "source": [
    "### **SET UP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | avg_val_loss: -0.0072\n",
      "epoch: 1 | avg_val_loss: -0.0048\n",
      "epoch: 2 | avg_val_loss: -0.0084\n",
      "epoch: 3 | avg_val_loss: -0.0061\n",
      "epoch: 4 | avg_val_loss: -0.0068\n",
      "epoch: 5 | avg_val_loss: -0.0040\n",
      "epoch: 6 | avg_val_loss: -0.0069\n",
      "epoch: 7 | avg_val_loss: -0.0027\n",
      "epoch: 8 | avg_val_loss: -0.0089\n",
      "epoch: 9 | avg_val_loss: -0.0083\n",
      "epoch: 10 | avg_val_loss: -0.0059\n",
      "epoch: 11 | avg_val_loss: -0.0037\n",
      "epoch: 12 | avg_val_loss: -0.0052\n",
      "epoch: 13 | avg_val_loss: -0.0066\n",
      "epoch: 14 | avg_val_loss: -0.0064\n",
      "epoch: 15 | avg_val_loss: -0.0051\n",
      "epoch: 16 | avg_val_loss: -0.0055\n",
      "epoch: 17 | avg_val_loss: -0.0048\n",
      "epoch: 18 | avg_val_loss: -0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiosh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 | avg_val_loss: -0.0072\n",
      "epoch: 20 | avg_val_loss: -0.0062\n",
      "epoch: 21 | avg_val_loss: -0.0059\n",
      "epoch: 22 | avg_val_loss: -0.0050\n",
      "epoch: 23 | avg_val_loss: -0.0080\n",
      "epoch: 24 | avg_val_loss: -0.0062\n",
      "epoch: 25 | avg_val_loss: -0.0073\n",
      "epoch: 26 | avg_val_loss: -0.0061\n",
      "epoch: 27 | avg_val_loss: -0.0087\n",
      "epoch: 28 | avg_val_loss: -0.0071\n",
      "epoch: 29 | avg_val_loss: -0.0039\n"
     ]
    }
   ],
   "source": [
    "meta_iteration = 30\n",
    "steps = 64\n",
    "num_tasks = 10\n",
    "\n",
    "train_loop(meta_iteration, steps, num_tasks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
