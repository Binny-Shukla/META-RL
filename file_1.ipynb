{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c12ae83",
   "metadata": {},
   "source": [
    "# RLÂ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1170db",
   "metadata": {},
   "source": [
    "### D E V I C E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a9fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f267c",
   "metadata": {},
   "source": [
    "### L O G G I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf818b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './runs/ppo_meta_rl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa108a0",
   "metadata": {},
   "source": [
    "### H E L P E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236b2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)\n",
    "\n",
    "def safe_stack(x):\n",
    "    \n",
    "    return torch.stack(x).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e44f5",
   "metadata": {},
   "source": [
    "### M E T A - E N V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90b9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Walker2DMetaEnv:\n",
    "    def __init__(self, task_batch=5):\n",
    "        self.base_env = gym.make(\"Walker2d-v5\")\n",
    "        self.task_batch = task_batch\n",
    "        self.target_velocity = 1.0\n",
    "\n",
    "    def sample_tasks(self, num_tasks):\n",
    "        self.tasks = []\n",
    "        for _ in range(num_tasks):\n",
    "            gravity = np.random.uniform(5.0, 15.0)\n",
    "            torso_mass = np.random.uniform(1.0, 5.0)\n",
    "            target_velocity = np.random.uniform(0.5, 3.0)\n",
    "            self.tasks.append((gravity, torso_mass, target_velocity))\n",
    "            \n",
    "        return self.tasks\n",
    "    \n",
    "\n",
    "    def set_task(self, task):\n",
    "        \n",
    "        raw_env = self.base_env.unwrapped\n",
    "        raw_env.model.opt.gravity[-1] = -task[0]\n",
    "        raw_env.model.body_mass[1] = task[1]\n",
    "        self.target_velocity = task[2]\n",
    "\n",
    "    def reset(self, task_idx=0):\n",
    "        self.set_task(self.tasks[task_idx])\n",
    "        obs, _ = self.base_env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.base_env.step(action)\n",
    "        raw_env = self.base_env.unwrapped\n",
    "        vel = raw_env.data.qvel[0]\n",
    "        reward -= 0.5 * abs(vel - self.target_velocity)\n",
    "        done = terminated or truncated\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def get_numbers(self):\n",
    "        \n",
    "        state_dim = self.base_env.observation_space.shape[0]\n",
    "        action_dim = self.base_env.action_space.shape[0]\n",
    "        max_action = self.base_env.action_space.high[0]\n",
    "        rewards_dim = 1\n",
    "        \n",
    "        return state_dim, action_dim, max_action, rewards_dim\n",
    "\n",
    "    def close(self):\n",
    "        self.base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5d2e8",
   "metadata": {},
   "source": [
    "### T E S T - E N V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0185da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial obs shape: (17,)\n",
      "state dim: 17 | action dim: 6 | max action: 1.0 | rewards dim: 1\n"
     ]
    }
   ],
   "source": [
    "env = Walker2DMetaEnv(task_batch=3)\n",
    "\n",
    "env.sample_tasks(num_tasks = 4)\n",
    "\n",
    "obs = env.reset(task_idx=0)\n",
    "\n",
    "print(\"Initial obs shape:\", np.shape(obs))\n",
    "\n",
    "state_dim, action_dim, max_action, rewards_dim = env.get_numbers()\n",
    "\n",
    "print(f'state dim: {state_dim} |'\n",
    "      f' action dim: {action_dim} |'\n",
    "      f' max action: {max_action} |'\n",
    "      f' rewards dim: {rewards_dim}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b2977",
   "metadata": {},
   "source": [
    "### A S S E M B L Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb46d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_1 = 128\n",
    "head_2 = 256\n",
    "head_3 = 256\n",
    "head_4 = 256\n",
    "head_5 = 128\n",
    "\n",
    "hidden_size = 128\n",
    "hidden_size_2 = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f392d3",
   "metadata": {},
   "source": [
    "### F E A T U R E "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6693ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_size = hidden_size, hidden_size_2 = hidden_size_2):\n",
    "        super(Feature_Extractor, self).__init__()\n",
    "        \n",
    "        self.extract = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(hidden_size_2),\n",
    "            nn.Linear(hidden_size_2, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, output_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.extract(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2cf23",
   "metadata": {},
   "source": [
    "### R E C U R R E N T -  P O L I C Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05311b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class recurrent_policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim = state_dim, action_dim = action_dim, rewards_dim = rewards_dim, head_1 = head_1, head_2 = head_2, head_3 = head_3, head_4 = head_4, head_5 = head_5, max_action = max_action):\n",
    "        super(recurrent_policy, self).__init__()\n",
    "        \n",
    "        # feature\n",
    "        \n",
    "        self.feature = Feature_Extractor(state_dim + action_dim + rewards_dim, head_1)\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        # seperate LSTM Layers\n",
    "        \n",
    "        self.actor_lstm = nn.LSTM(head_1, head_2, num_layers = 2, batch_first = True)\n",
    "        \n",
    "        self.critic_lstm = nn.LSTM(head_1, head_2, num_layers = 2, batch_first = True)\n",
    "        \n",
    "        # mlp\n",
    "        \n",
    "        def create_mlp():\n",
    "            \n",
    "            process = nn.Sequential(\n",
    "                \n",
    "                nn.Linear(head_2, head_3),\n",
    "                nn.SiLU(),\n",
    "                \n",
    "                nn.LayerNorm(head_3),\n",
    "                nn.Linear(head_3, head_4),\n",
    "                nn.SiLU(),\n",
    "                \n",
    "                nn.Linear(head_4, head_5),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "            \n",
    "            return process\n",
    "            \n",
    "        # post feature\n",
    "        \n",
    "        self.actor_post_feature = create_mlp()\n",
    "        self.critic_post_feature = create_mlp()\n",
    "        \n",
    "        # mu and log head\n",
    "        \n",
    "        self.mu = nn.Linear(head_5, action_dim)\n",
    "        self.log_std = nn.Linear(head_5, action_dim)\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        self.critic_head = nn.Linear(head_5, 1)\n",
    "        \n",
    "        # max action \n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Stabilization\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "        \n",
    "    def init_weights(self, module):\n",
    "            \n",
    "        if isinstance(module, nn.Linear):\n",
    "                \n",
    "            nn.init.kaiming_normal_(module.weight, a = 0, nonlinearity = 'relu')\n",
    "                \n",
    "            if module.bias is not None:\n",
    "                    \n",
    "                nn.init.zeros_(module.bias)\n",
    "                    \n",
    "                    \n",
    "        elif isinstance(module, nn.LSTM):\n",
    "                \n",
    "            for name, param in module.named_parameters():\n",
    "                    \n",
    "                if 'weights_ih' in name:\n",
    "                        \n",
    "                    nn.init.kaiming_uniform_(param, math.sqrt(5))\n",
    "                        \n",
    "                elif 'weights_hh' in name:\n",
    "                        \n",
    "                    nn.init.orthogonal_(param)\n",
    "                        \n",
    "                elif 'bias' in name:\n",
    "                        \n",
    "                    nn.init.zeros_(param)\n",
    "        \n",
    "    def forward(self, state, prev_action, prev_reward, actor_memory = None, critic_memory = None):\n",
    "        \n",
    "        # cat\n",
    "        \n",
    "        cat = torch.cat([state, prev_action, prev_reward], dim = -1)\n",
    "        \n",
    "        # feature \n",
    "        \n",
    "        feature = self.feature(cat)\n",
    "        \n",
    "        # norm\n",
    "        \n",
    "        norm = self.norm(feature)\n",
    "        \n",
    "        # lstm layers\n",
    "        \n",
    "        if norm.dim() == 2:\n",
    "            norm = norm.unsqueeze(1)\n",
    "        \n",
    "            actor_lstm_out, h_a = self.actor_lstm(norm, actor_memory)\n",
    "            critic_lstm_out, h_c = self.critic_lstm(norm, critic_memory)\n",
    "            \n",
    "            actor_lstm_out = actor_lstm_out.squeeze(1)\n",
    "            critic_lstm_out = critic_lstm_out.squeeze(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            actor_lstm_out, h_a = self.actor_lstm(norm, actor_memory)\n",
    "            critic_lstm_out, h_c = self.critic_lstm(norm, critic_memory)\n",
    "        \n",
    "        # post feature\n",
    "        \n",
    "        actor_post_feature = self.actor_post_feature(actor_lstm_out)\n",
    "        \n",
    "        critic_post_feature = self.critic_post_feature(critic_lstm_out)\n",
    "        \n",
    "        # critic head\n",
    "        \n",
    "        critic_val = self.critic_head(critic_post_feature)\n",
    "        \n",
    "        # actor head\n",
    "        \n",
    "        mu = self.mu(actor_post_feature)\n",
    "        log_std = self.log_std(actor_post_feature)\n",
    "        \n",
    "        log_std = torch.clamp(log_std, -10, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        z = dist.rsample()\n",
    "        tanh_z = torch.tanh(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        log_prob = dist.log_prob(z).sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        squash = (1 - tanh_z.pow(2) + 1e-6).log().sum(dim = -1, keepdim = True)\n",
    "        \n",
    "        log_prob = log_prob - squash\n",
    "        \n",
    "        return action, log_prob, mu, log_std, critic_val, h_a, h_c\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, ):\n",
    "        \n",
    "        h_a = torch.zeros(2, 1, self.actor_lstm.hidden_size).to(next(self.parameters()))\n",
    "        c_a = torch.zeros(2, 1, self.actor_lstm.hidden_size).to(next(self.parameters()))\n",
    "        \n",
    "        h_c = torch.zeros(2, 1, self.critic_lstm.hidden_size).to(next(self.parameters()))\n",
    "        c_c = torch.zeros(2, 1, self.critic_lstm.hidden_size).to(next(self.parameters()))\n",
    "        \n",
    "        return (h_a, c_a), (h_c, c_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca948112",
   "metadata": {},
   "source": [
    "### S E T U P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f442013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recurrent_policy(\n",
      "  (feature): Feature_Extractor(\n",
      "    (extract): Sequential(\n",
      "      (0): Linear(in_features=24, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (4): SiLU()\n",
      "      (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (7): SiLU()\n",
      "      (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (9): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (actor_lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (critic_lstm): LSTM(128, 256, num_layers=2, batch_first=True)\n",
      "  (actor_post_feature): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (6): SiLU()\n",
      "  )\n",
      "  (critic_post_feature): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (6): SiLU()\n",
      "  )\n",
      "  (mu): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (log_std): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (critic_head): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "RECURRENT_NETWORK = recurrent_policy().to(device)\n",
    "\n",
    "print(RECURRENT_NETWORK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dfa498",
   "metadata": {},
   "source": [
    "### B U F F E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "541782a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class roller_buffer:\n",
    "    \n",
    "    def __init__(self, max_episodes):\n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.episodes = []\n",
    "        self.current_episode = []\n",
    "        \n",
    "        \n",
    "    def add(self, state, action, log_prob, reward, done, h, c, next_state):\n",
    "        \n",
    "        # convert to tensor\n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = action.detach()\n",
    "        log_prob = log_prob.detach()\n",
    "        reward = safe_tensor(reward)\n",
    "        done = safe_tensor(done)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        \n",
    "        h_a = h[0]\n",
    "        c_a = h[1]\n",
    "        h_c = c[0]\n",
    "        c_c = c[1]\n",
    "\n",
    "        self.current_episode.append({\n",
    "            \n",
    "            'states': state,\n",
    "            'actions': action,\n",
    "            'log_probs': log_prob,\n",
    "            'rewards': reward,\n",
    "            'dones': done,\n",
    "            'h_a': h_a,\n",
    "            'c_a': c_a,\n",
    "            'h_c': h_c,\n",
    "            'c_c': c_c,\n",
    "            'next_states': next_state\n",
    "        })\n",
    "        \n",
    "        if done.item() == 1:\n",
    "            \n",
    "            self.episodes.append(self.current_episode)\n",
    "            self.current_episode = []\n",
    "            \n",
    "            if len(self.episodes) > self.max_episodes:\n",
    "                \n",
    "                self.episodes.pop(0)\n",
    "        \n",
    "    def sample(self, seq_length, batch_size):\n",
    "        \n",
    "        segments = []\n",
    "        masks = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            \n",
    "            ep = np.random.choice(self.episodes)\n",
    "            \n",
    "            if len(ep) < seq_length:\n",
    "                \n",
    "                padded, mask = self.pad_episode(ep, seq_length)\n",
    "                segments.append(padded)\n",
    "                masks.append(mask)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                start = np.random.randint(0, len(ep) - seq_length + 1)\n",
    "                segment = ep[start: start + seq_length]\n",
    "                mask = torch.ones(seq_length, dtype = torch.float32)\n",
    "                segments.append(segment)\n",
    "                masks.append(mask)       \n",
    "                \n",
    "        batch = {\n",
    "            \n",
    "            'states': safe_stack([safe_stack([s['states'] for s in seg]) for seg in segments]),\n",
    "            'actions': safe_stack([safe_stack([s['actions'] for s in seg]) for seg in segments]),\n",
    "            'log_probs': safe_stack([safe_stack([s['log_probs'] for s in seg]) for seg in segments]),\n",
    "            'rewards': safe_stack([safe_stack([s['rewards'] for s in seg]) for seg in segments]),\n",
    "            'dones': safe_stack([safe_stack([s['dones'] for s in seg]) for seg in segments]),\n",
    "            'h_a': torch.stack([torch.stack([s['h_a'] for s in seg], dim=0) for seg in segments], dim=1),\n",
    "            'c_a': torch.stack([torch.stack([s['c_a'] for s in seg], dim=0) for seg in segments], dim=1),\n",
    "            'h_c': torch.stack([torch.stack([s['h_c'] for s in seg], dim=0) for seg in segments], dim=1),\n",
    "            'c_c': torch.stack([torch.stack([s['c_c'] for s in seg], dim=0) for seg in segments], dim=1),\n",
    "            'next_states': safe_stack([safe_stack([s['next_states'] for s in seg]) for seg in segments]),\n",
    "            'masks': safe_stack(masks)\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "                \n",
    "    def pad_episode(self, ep, seq_length):\n",
    "        \n",
    "        pad_length = seq_length - len(ep)\n",
    "        \n",
    "        last_step = ep[-1]\n",
    "        \n",
    "        pad_step = {}\n",
    "        \n",
    "        for k, v in last_step.items():\n",
    "            if torch.is_tensor(v):\n",
    "                pad_step[k] = torch.zeros_like(v)\n",
    "            elif isinstance(v, tuple):  # handle LSTM hidden states\n",
    "                pad_step[k] = tuple(torch.zeros_like(t) for t in v)\n",
    "            else:\n",
    "                pad_step[k] = v \n",
    "                \n",
    "        mask = torch.cat([\n",
    "            \n",
    "            torch.ones(len(ep), dtype = torch.float32),\n",
    "            torch.zeros(pad_length, dtype = torch.float32)\n",
    "        ])\n",
    "        \n",
    "        return ep + [pad_step] * pad_length, mask\n",
    "        \n",
    "        \n",
    "    def clear(self):\n",
    "        \n",
    "        ''' a logic to clear enough data to do no make the agent to suffocate and explode my ram'''\n",
    "        \n",
    "        self.episodes.clear()\n",
    "        self.current_episode.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3376c3",
   "metadata": {},
   "source": [
    "### S E T U P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01c8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### setup\n",
    "\n",
    "max_episodes = 10\n",
    "\n",
    "buffer = roller_buffer(max_episodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f72884",
   "metadata": {},
   "source": [
    "### M E T A - E P I S O D E - R U N N E R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3265f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_episode_runner:\n",
    "    \n",
    "    def __init__(self, max_episode_length, buffer = buffer, agent = RECURRENT_NETWORK, env = env):\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.buffer = buffer\n",
    "        self.env = env\n",
    "        self.max_episode_length = max_episode_length\n",
    "        \n",
    "        \n",
    "    def run(self, num_tasks):\n",
    "        \n",
    "        tasks = self.env.sample_tasks(num_tasks)\n",
    "        \n",
    "        for task in tasks:\n",
    "            \n",
    "            self.env.set_task(task)\n",
    "            obs = self.env.reset()\n",
    "            obs = safe_tensor(obs)\n",
    "            if obs.dim() == 1: obs = obs.unsqueeze(0)\n",
    "            \n",
    "            # Init LSTM states\n",
    "            \n",
    "            (h_a, c_a), (h_c, c_c) = self.agent.init_hidden()\n",
    "\n",
    "            \n",
    "            prev_action = torch.zeros(1, action_dim).to(device)\n",
    "            prev_reward = torch.zeros(1, 1).to(device)\n",
    "            \n",
    "            # episode data\n",
    "            \n",
    "            for t in range(self.max_episode_length):\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    action, log_prob, _, _, _, next_h, next_c = self.agent(obs, prev_action, prev_reward, actor_memory = (h_a, c_a), critic_memory = (h_c, c_c))\n",
    "                    \n",
    "                action_np = action.cpu().numpy()[0]\n",
    "                    \n",
    "                next_obs, reward, done, _ = self.env.step(action_np)\n",
    "                next_obs = safe_tensor(next_obs)\n",
    "                if next_obs.dim() == 1: next_obs = next_obs.unsqueeze(0)\n",
    "                h = (h_a, c_a) \n",
    "                c = (h_c, c_c)\n",
    "                    \n",
    "                self.buffer.add(obs.squeeze(0), action.squeeze(0), log_prob.squeeze(0), reward, done, h, c, next_obs.squeeze(0))\n",
    "                    \n",
    "                obs = next_obs\n",
    "                h_a, c_a = next_h\n",
    "                h_c, c_c = next_c\n",
    "                     \n",
    "                if done:\n",
    "                        \n",
    "                    break\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f587b5",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a55888",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_length = 512\n",
    "\n",
    "\n",
    "META_RUNNER = meta_episode_runner(max_episode_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a01267",
   "metadata": {},
   "source": [
    "### O P T I M I Z E R "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5901aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### shared feature extractor\n",
    "\n",
    "shared_lr = 1e-4\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 5e-4\n",
    "T_max = 10\n",
    "\n",
    "# params\n",
    "\n",
    "actor_param = list(RECURRENT_NETWORK.actor_lstm.parameters()) + \\\n",
    "              list(RECURRENT_NETWORK.actor_post_feature.parameters()) + \\\n",
    "              list(RECURRENT_NETWORK.mu.parameters()) + \\\n",
    "              list(RECURRENT_NETWORK.log_std.parameters())\n",
    "              \n",
    "critic_param = list(RECURRENT_NETWORK.critic_lstm.parameters()) + \\\n",
    "               list(RECURRENT_NETWORK.critic_post_feature.parameters()) + \\\n",
    "               list(RECURRENT_NETWORK.critic_head.parameters())\n",
    "               \n",
    "# optimizer\n",
    "\n",
    "OPTIMIZER = optim.AdamW([\n",
    "    \n",
    "    {'params': RECURRENT_NETWORK.feature.parameters(), 'lr': shared_lr},\n",
    "    {'params': actor_param, 'lr': actor_lr},\n",
    "    {'params': critic_param, 'lr': critic_lr}\n",
    "    \n",
    "], weight_decay = 0)\n",
    "\n",
    "# Scheduler\n",
    "\n",
    "SCHEDULER = optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max, eta_min = 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee14b47",
   "metadata": {},
   "source": [
    "### L O S S - F U N C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3946c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    \n",
    "    def __init__(self, gamma, gae_lam, entropy_coef, clip_epsilon, value_coef, RECURRENT_NETWORK = RECURRENT_NETWORK, buffer = buffer, OPTIMIZER = OPTIMIZER, SCHEDULER = SCHEDULER):\n",
    "        \n",
    "        # network\n",
    "        \n",
    "        self.network = RECURRENT_NETWORK\n",
    "        \n",
    "        # optimizer\n",
    "        \n",
    "        self.optimizer = OPTIMIZER\n",
    "        self.scheduler = SCHEDULER\n",
    "        \n",
    "        # buffer\n",
    "        \n",
    "        self.buffer = buffer\n",
    "        \n",
    "        # hyper params\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lam = gae_lam\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        \n",
    "    def compute_gae(self, rewards, dones, value, last_value):\n",
    "        \n",
    "        values = torch.cat([value, last_value], dim = 1).to(device)\n",
    "        \n",
    "        gae = 0\n",
    "        advantages = []\n",
    "        \n",
    "        for step in reversed(range(rewards.shape[1])):\n",
    "            \n",
    "            delta = rewards[:, step] + self.gamma * (1 - dones[:, step]) * values[:, step + 1] - values[:, step]\n",
    "            gae = delta + self.gamma * self.gae_lam * (1 - dones[:, step]) * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        advantages = safe_stack(advantages)\n",
    "        \n",
    "        returns = advantages + value\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        return advantages.detach(), returns.detach()\n",
    "        \n",
    "    def critic_loss(self, value, returns, mask):\n",
    "        \n",
    "        error = F.mse_loss(value, returns)\n",
    "    \n",
    "        if mask is not None:\n",
    "            \n",
    "            error = error * mask\n",
    "            error = error.sum() / mask.sum()\n",
    "            \n",
    "        return error\n",
    "    \n",
    "    def compute_surrogate_loss(self, old_log_probs, log_probs, advantages):\n",
    "        \n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        \n",
    "        surr_1 = (ratio * advantages)\n",
    "        surr_2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "        \n",
    "        surrogate_loss = -torch.min(surr_1, surr_2).mean()\n",
    "        \n",
    "        return surrogate_loss\n",
    "    \n",
    "    def shape_corrector(self, rewards, dones, h_a, h_c, c_a, c_c):\n",
    "        \n",
    "        if rewards.dim() == 2: rewards = rewards.unsqueeze(2)\n",
    "        if dones.dim() == 2: dones = dones.unsqueeze(2)\n",
    "        if h_a.dim() == 5: h_a = h_a.squeeze(3)\n",
    "        if h_c.dim() == 5: h_c = h_c.squeeze(3)\n",
    "        if c_a.dim() == 5: c_a = c_a.squeeze(3)\n",
    "        if c_c.dim() == 5: c_c = c_c.squeeze(3)\n",
    "        \n",
    "        h_a = h_a[:, 0].permute(1, 0, 2).contiguous()\n",
    "        c_a = c_a[:, 0].permute(1, 0, 2).contiguous()\n",
    "        h_c = h_c[:, 0].permute(1, 0, 2).contiguous()\n",
    "        c_c = c_c[:, 0].permute(1, 0, 2).contiguous()\n",
    "        \n",
    "        return rewards, dones, h_a, h_c, c_a, c_c\n",
    "    \n",
    "    def update(self, seq_length, batch_size):\n",
    "        \n",
    "        # sample the batch \n",
    "        \n",
    "        batch = self.buffer.sample(seq_length, batch_size)\n",
    "        \n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        old_log_probs = batch['log_probs']\n",
    "        rewards = batch['rewards']\n",
    "        dones = batch['dones']\n",
    "        h_a = batch['h_a']\n",
    "        c_a = batch['c_a']\n",
    "        h_c = batch['h_c']\n",
    "        c_c = batch['c_c']\n",
    "        masks = batch['masks']\n",
    "        next_states = batch['next_states']\n",
    "        \n",
    "        # shape check\n",
    "        \n",
    "        rewards, dones, h_a, h_c, c_a, c_c = self.shape_corrector(rewards, dones, h_a, h_c, c_a, c_c)\n",
    "        \n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "        \n",
    "        final_state = next_states[:, -1:]\n",
    "        final_prev_action = actions[:, -1:]\n",
    "        \n",
    "        final_prev_reward = rewards[:, -1:]\n",
    "        \n",
    "        h = (h_a, c_a)\n",
    "        c = (h_c, c_c)\n",
    "        \n",
    "        # compute value, current log probs, last_value\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            _, _, _, _, last_value, _, _ = self.network(final_state, final_prev_action, final_prev_reward, h, c)\n",
    "            \n",
    "        _, log_probs, mu, log_std, values, _, _ = self.network(states, actions, rewards, h, c)\n",
    "        \n",
    "        # compute GAE\n",
    "        \n",
    "        advantages, returns = self.compute_gae(rewards, dones, values, last_value)\n",
    "        \n",
    "        # compute critic loss\n",
    "        \n",
    "        critic_loss = self.critic_loss(values, returns, masks)\n",
    "        critic_loss = critic_loss * self.value_coef\n",
    "        \n",
    "        # compute policy loss\n",
    "        \n",
    "        policy_loss = self.compute_surrogate_loss(old_log_probs, log_probs, advantages)\n",
    "        \n",
    "        # entropy loss\n",
    "        \n",
    "        dist = torch.distributions.Normal(mu, torch.exp(log_std))\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        total_loss = policy_loss- self.entropy_coef * entropy + critic_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm = 0.1)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        \n",
    "        return total_loss.item(), policy_loss.item(), critic_loss.item()       \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d002c272",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c082894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "\n",
    "gamma = 0.99\n",
    "gae_lam = 0.95\n",
    "entropy_coef = 0.01\n",
    "value_coef = 0.25\n",
    "clip_epsilon = 0.2\n",
    "\n",
    "# setup\n",
    "\n",
    "LOSS_FUNCTION = loss_func(gamma, gae_lam, entropy_coef, clip_epsilon, value_coef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60cb85",
   "metadata": {},
   "source": [
    "### T R A I N I N G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc61ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, mini_batch, num_tasks, seq_length, batch_size, RECURRENT_NETWORK = RECURRENT_NETWORK, LOSS_FUNCTION = LOSS_FUNCTION, META_RUNNER = META_RUNNER):\n",
    "    \n",
    "    RECURRENT_NETWORK.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        META_RUNNER.run(num_tasks)\n",
    "    \n",
    "        total_policy_loss, total_value_loss, total_agent_loss = 0.0, 0.0, 0.0\n",
    "        \n",
    "        for _ in range(mini_batch):\n",
    "            \n",
    "            total_loss, policy_loss, critic_loss = LOSS_FUNCTION.update(seq_length, batch_size)\n",
    "    \n",
    "            total_policy_loss += policy_loss\n",
    "            total_value_loss += critic_loss\n",
    "            total_agent_loss += total_loss\n",
    "            \n",
    "        avg_total_loss = total_agent_loss / mini_batch\n",
    "        avg_policy_loss = total_policy_loss / mini_batch\n",
    "        avg_value_loss = total_value_loss / mini_batch\n",
    "        \n",
    "        writer.add_scalar('Agent loss', avg_total_loss, epoch)\n",
    "        writer.add_scalar('Policy loss', avg_policy_loss, epoch)\n",
    "        writer.add_scalar('Value loss', avg_value_loss, epoch)\n",
    "        \n",
    "        writer.flush()\n",
    "        \n",
    "        print(f'epoch: {epoch} | policy loss: {avg_policy_loss:.3f} | avg value loss: {avg_value_loss:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90beb834",
   "metadata": {},
   "source": [
    "### S E T U P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e565ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiosh\\AppData\\Local\\Temp\\ipykernel_9948\\1075654719.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ep = np.random.choice(self.episodes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | policy loss: 37921.911 | avg value loss: 0.357\n",
      "epoch: 1 | policy loss: 0.151 | avg value loss: 0.258\n",
      "epoch: 2 | policy loss: 0.119 | avg value loss: 0.250\n",
      "epoch: 3 | policy loss: 0.105 | avg value loss: 0.249\n",
      "epoch: 4 | policy loss: 0.066 | avg value loss: 0.250\n",
      "epoch: 5 | policy loss: 0.061 | avg value loss: 0.251\n",
      "epoch: 6 | policy loss: 0.055 | avg value loss: 0.256\n",
      "epoch: 7 | policy loss: 0.065 | avg value loss: 0.213\n",
      "epoch: 8 | policy loss: 0.064 | avg value loss: 0.242\n",
      "epoch: 9 | policy loss: 0.051 | avg value loss: 0.218\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "mini_batch = 64\n",
    "batch_size = 256\n",
    "seq_length = 256\n",
    "num_tasks = 4\n",
    "\n",
    "train(epochs, mini_batch, num_tasks, seq_length, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
